{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# import pkg_resources\n",
        "# pkg_resources.require('Torch==2.0.1')\n",
        "!pip install torch==2.0.1\n",
        "import torch\n",
        "print('Using torch', torch.__version__)\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install ogb\n",
        "!pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4tixkw6XbF1",
        "outputId": "aea7754f-e8d6-4597-ac69-ff782a696345"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (17.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Using torch 2.0.1+cu117\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt20cu118)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt20cu118)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt20cu118)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt20cu118)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->ogb) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (17.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FPGNN\n",
        "\n",
        "from argparse import Namespace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "# from fpgnn.data import GetPubChemFPs, create_graph, get_atom_features_dim\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# required functions for our model\n",
        "\n",
        "def remove_non_alphanumeric(input_string):\n",
        "    # Use a regular expression to match only letters and numbers\n",
        "    alphanumeric_pattern = re.compile(r'[\\W_]+')\n",
        "\n",
        "    # Replace non-alphanumeric characters with an empty string\n",
        "    result_string = alphanumeric_pattern.sub('', input_string)\n",
        "\n",
        "    return result_string\n",
        "\n",
        "atom_type_max = 100\n",
        "atom_f_dim = 133\n",
        "atom_features_define = {\n",
        "    'atom_symbol': list(range(atom_type_max)),\n",
        "    'degree': [0, 1, 2, 3, 4, 5],\n",
        "    'formal_charge': [-1, -2, 1, 2, 0],\n",
        "    'charity_type': [0, 1, 2, 3],\n",
        "    'hydrogen': [0, 1, 2, 3, 4],\n",
        "    'hybridization': [\n",
        "        Chem.rdchem.HybridizationType.SP,\n",
        "        Chem.rdchem.HybridizationType.SP2,\n",
        "        Chem.rdchem.HybridizationType.SP3,\n",
        "        Chem.rdchem.HybridizationType.SP3D,\n",
        "        Chem.rdchem.HybridizationType.SP3D2\n",
        "    ],}\n",
        "\n",
        "smile_changed = {}\n",
        "\n",
        "def get_atom_features_dim():\n",
        "    return atom_f_dim\n",
        "\n",
        "def onek_encoding_unk(key,length):\n",
        "    encoding = [0] * (len(length) + 1)\n",
        "    index = length.index(key) if key in length else -1\n",
        "    encoding[index] = 1\n",
        "\n",
        "    return encoding\n",
        "\n",
        "def get_atom_feature(atom):\n",
        "    feature = onek_encoding_unk(atom.GetAtomicNum() - 1, atom_features_define['atom_symbol']) + \\\n",
        "           onek_encoding_unk(atom.GetTotalDegree(), atom_features_define['degree']) + \\\n",
        "           onek_encoding_unk(atom.GetFormalCharge(), atom_features_define['formal_charge']) + \\\n",
        "           onek_encoding_unk(int(atom.GetChiralTag()), atom_features_define['charity_type']) + \\\n",
        "           onek_encoding_unk(int(atom.GetTotalNumHs()), atom_features_define['hydrogen']) + \\\n",
        "           onek_encoding_unk(int(atom.GetHybridization()), atom_features_define['hybridization']) + \\\n",
        "           [1 if atom.GetIsAromatic() else 0] + \\\n",
        "           [atom.GetMass() * 0.01]\n",
        "    return feature\n",
        "\n",
        "class GraphOne:\n",
        "    def __init__(self,smile,args):\n",
        "        self.smile = smile\n",
        "        self.atom_feature = []\n",
        "\n",
        "        mol = Chem.MolFromSmiles(self.smile)\n",
        "        # condition is added by me\n",
        "        if mol is None:\n",
        "          # print('baddddddd')\n",
        "          smile = 'N#CCC1=CC=CC=C1'\n",
        "          self.smile = smile\n",
        "          mol = Chem.MolFromSmiles(self.smile)\n",
        "        else:\n",
        "          # print('good')\n",
        "          blood = 7\n",
        "        self.atom_num = mol.GetNumAtoms()\n",
        "\n",
        "        for i, atom in enumerate(mol.GetAtoms()):\n",
        "            self.atom_feature.append(get_atom_feature(atom))\n",
        "        self.atom_feature = [self.atom_feature[i] for i in range(self.atom_num)]\n",
        "\n",
        "class GraphBatch:\n",
        "    def __init__(self,graphs,args):\n",
        "        smile_list = []\n",
        "        for graph in graphs:\n",
        "            smile_list.append(graph.smile)\n",
        "        self.smile_list = smile_list\n",
        "        self.smile_num = len(self.smile_list)\n",
        "        self.atom_feature_dim = get_atom_features_dim()\n",
        "        self.atom_no = 1\n",
        "        self.atom_index = []\n",
        "\n",
        "        atom_feature = [[0]*self.atom_feature_dim]\n",
        "        for graph in graphs:\n",
        "            atom_feature.extend(graph.atom_feature)\n",
        "            self.atom_index.append((self.atom_no,graph.atom_num))\n",
        "            self.atom_no += graph.atom_num\n",
        "\n",
        "        self.atom_feature = torch.FloatTensor(atom_feature)\n",
        "\n",
        "    def get_feature(self):\n",
        "        return self.atom_feature,self.atom_index\n",
        "\n",
        "def create_graph(smile,args):\n",
        "    graphs = []\n",
        "    for one in smile:\n",
        "        if one in smile_changed:\n",
        "            graph = smile_changed[one]\n",
        "        else:\n",
        "            graph = GraphOne(one, args)\n",
        "            smile_changed[one] = graph\n",
        "        graphs.append(graph)\n",
        "    return GraphBatch(graphs,args)\n",
        "\n",
        "# our model is shown below\n",
        "\n",
        "atts_out = []\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(FPN, self).__init__()\n",
        "        self.fp_2_dim=args.fp_2_dim\n",
        "        self.dropout_fpn = args.dropout\n",
        "        self.cuda = args.cuda\n",
        "        self.hidden_dim = args.hidden_size\n",
        "        self.args = args\n",
        "        if hasattr(args,'fp_type'):\n",
        "            self.fp_type = args.fp_type\n",
        "        else:\n",
        "            self.fp_type = 'mixed'\n",
        "\n",
        "        if self.fp_type == 'mixed':\n",
        "            self.fp_dim = 1489\n",
        "        else:\n",
        "            self.fp_dim = 1024\n",
        "\n",
        "        if hasattr(args,'fp_changebit'):\n",
        "            self.fp_changebit = args.fp_changebit\n",
        "        else:\n",
        "            self.fp_changebit = None\n",
        "\n",
        "        self.fc1=nn.Linear(self.fp_dim, self.fp_2_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.fp_2_dim, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout_fpn)\n",
        "\n",
        "    def forward(self, smile):\n",
        "        fp_list=[]\n",
        "        for i, one in enumerate(smile):\n",
        "            fp=[]\n",
        "            mol = Chem.MolFromSmiles(one)\n",
        "\n",
        "            if self.fp_type == 'mixed':\n",
        "                fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)\n",
        "                fp_phaErGfp = AllChem.GetErGFingerprint(mol,fuzzIncrement=0.3,maxPath=21,minPath=1)\n",
        "                # fp_pubcfp = GetPubChemFPs(mol)\n",
        "                fp.extend(fp_maccs)\n",
        "                fp.extend(fp_phaErGfp)\n",
        "                # fp.extend(fp_pubcfp)\n",
        "            else:\n",
        "                fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
        "                fp.extend(fp_morgan)\n",
        "            fp_list.append(fp)\n",
        "\n",
        "        if self.fp_changebit is not None and self.fp_changebit != 0:\n",
        "            fp_list = np.array(fp_list)\n",
        "            fp_list[:,self.fp_changebit-1] = np.ones(fp_list[:,self.fp_changebit-1].shape)\n",
        "            fp_list.tolist()\n",
        "\n",
        "        fp_list = torch.Tensor(fp_list)\n",
        "\n",
        "        if self.cuda:\n",
        "            fp_list = fp_list.cuda()\n",
        "        fpn_out = self.fc1(fp_list)\n",
        "        fpn_out = self.dropout(fpn_out)\n",
        "        fpn_out = self.act_func(fpn_out)\n",
        "        fpn_out = self.fc2(fpn_out)\n",
        "        return fpn_out\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout_gnn, alpha, inter_graph, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout_gnn= dropout_gnn\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "        self.dropout = nn.Dropout(p=self.dropout_gnn)\n",
        "        self.inter_graph = inter_graph\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        if self.inter_graph is not None:\n",
        "            self.atts_out = []\n",
        "\n",
        "    def forward(self,mole_out,adj):\n",
        "        atom_feature = torch.mm(mole_out, self.W)\n",
        "        N = atom_feature.size()[0]\n",
        "\n",
        "        atom_trans = torch.cat([atom_feature.repeat(1, N).view(N * N, -1), atom_feature.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(atom_trans, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "        if self.inter_graph is not None:\n",
        "            att_out = attention\n",
        "            if att_out.is_cuda:\n",
        "                att_out = att_out.cpu()\n",
        "            att_out = np.array(att_out)\n",
        "            att_out[att_out<-10000] = 0\n",
        "            att_out = att_out.tolist()\n",
        "            atts_out.append(att_out)\n",
        "\n",
        "        attention = nn.functional.softmax(attention, dim=1)\n",
        "        attention = self.dropout(attention)\n",
        "        output = torch.matmul(attention, atom_feature)\n",
        "\n",
        "        if self.concat:\n",
        "            return nn.functional.elu(output)\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class GATOne(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GATOne, self).__init__()\n",
        "        self.nfeat = get_atom_features_dim()\n",
        "        self.nhid = args.nhid\n",
        "        self.dropout_gnn = args.dropout_gat\n",
        "        self.atom_dim = args.hidden_size\n",
        "        self.alpha = 0.2\n",
        "        self.nheads = args.nheads\n",
        "        self.args = args\n",
        "        self.dropout = nn.Dropout(p=self.dropout_gnn)\n",
        "\n",
        "        if hasattr(args,'inter_graph'):\n",
        "            self.inter_graph = args.inter_graph\n",
        "        else:\n",
        "            self.inter_graph = None\n",
        "\n",
        "        self.attentions = [GATLayer(self.nfeat, self.nhid, dropout_gnn=self.dropout_gnn, alpha=self.alpha, inter_graph=self.inter_graph, concat=True) for _ in range(self.nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GATLayer(self.nhid * self.nheads, self.atom_dim, dropout_gnn=self.dropout_gnn, alpha=self.alpha, inter_graph=self.inter_graph, concat=False)\n",
        "\n",
        "    def forward(self,mole_out,adj):\n",
        "        mole_out = self.dropout(mole_out)\n",
        "        mole_out = torch.cat([att(mole_out, adj) for att in self.attentions], dim=1)\n",
        "        mole_out = self.dropout(mole_out)\n",
        "        mole_out = nn.functional.elu(self.out_att(mole_out, adj))\n",
        "        return nn.functional.log_softmax(mole_out, dim=1)\n",
        "\n",
        "class GATEncoder(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GATEncoder,self).__init__()\n",
        "        self.cuda = args.cuda\n",
        "        self.args = args\n",
        "        self.encoder = GATOne(self.args)\n",
        "\n",
        "    def forward(self,mols,smiles):\n",
        "        atom_feature, atom_index = mols.get_feature()\n",
        "        if self.cuda:\n",
        "            atom_feature = atom_feature.cuda()\n",
        "\n",
        "        gat_outs=[]\n",
        "\n",
        "        for i,one in enumerate(smiles):\n",
        "            adj = []\n",
        "            mol = Chem.MolFromSmiles(one)\n",
        "            # added by me\n",
        "            if mol is None:\n",
        "              one = 'N#CCC1=CC=CC=C1'\n",
        "              mol = Chem.MolFromSmiles(one)\n",
        "            adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
        "            # print('broooooooo')\n",
        "            adj = adj/1\n",
        "            adj = torch.from_numpy(adj)\n",
        "            if self.cuda:\n",
        "                adj = adj.cuda()\n",
        "\n",
        "            atom_start, atom_size = atom_index[i]\n",
        "            one_feature = atom_feature[atom_start:atom_start+atom_size]\n",
        "\n",
        "            gat_atoms_out = self.encoder(one_feature,adj)\n",
        "            gat_out = gat_atoms_out.sum(dim=0)/atom_size\n",
        "            gat_outs.append(gat_out)\n",
        "        gat_outs = torch.stack(gat_outs, dim=0)\n",
        "        return gat_outs\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GAT,self).__init__()\n",
        "        self.args = args\n",
        "        self.encoder = GATEncoder(self.args)\n",
        "\n",
        "    def forward(self,smile):\n",
        "        mol = create_graph(smile, self.args)\n",
        "        gat_out = self.encoder.forward(mol,smile)\n",
        "\n",
        "        return gat_out\n",
        "\n",
        "class FpgnnModel(nn.Module):\n",
        "    def __init__(self,is_classif,gat_scale,cuda,dropout_fpn):\n",
        "        super(FpgnnModel, self).__init__()\n",
        "        self.gat_scale = gat_scale\n",
        "        self.is_classif = is_classif\n",
        "        self.cuda = cuda\n",
        "        self.dropout_fpn = dropout_fpn\n",
        "        if self.is_classif:\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def create_gat(self,args):\n",
        "        self.encoder3 = GAT(args)\n",
        "\n",
        "    def create_fpn(self,args):\n",
        "        self.encoder2 = FPN(args)\n",
        "\n",
        "    def create_scale(self,args):\n",
        "        linear_dim = int(args.hidden_size)\n",
        "        if self.gat_scale == 1:\n",
        "            self.fc_gat = nn.Linear(linear_dim,linear_dim)\n",
        "        elif self.gat_scale == 0:\n",
        "            self.fc_fpn = nn.Linear(linear_dim,linear_dim)\n",
        "        else:\n",
        "            self.gat_dim = int((linear_dim*2*self.gat_scale)//1)\n",
        "            self.fc_gat = nn.Linear(linear_dim,self.gat_dim)\n",
        "            self.fc_fpn = nn.Linear(linear_dim,linear_dim*2-self.gat_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "\n",
        "    def create_ffn(self,args):\n",
        "        linear_dim = args.hidden_size\n",
        "        if self.gat_scale == 1:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "        elif self.gat_scale == 0:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "\n",
        "        else:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim*2, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "\n",
        "    def forward(self,input):\n",
        "        if self.gat_scale == 1:\n",
        "            output = self.encoder3(input)\n",
        "        elif self.gat_scale == 0:\n",
        "            output = self.encoder2(input)\n",
        "        else:\n",
        "            gat_out = self.encoder3(input)\n",
        "            fpn_out = self.encoder2(input)\n",
        "            gat_out = self.fc_gat(gat_out)\n",
        "            gat_out = self.act_func(gat_out)\n",
        "\n",
        "            fpn_out = self.fc_fpn(fpn_out)\n",
        "            fpn_out = self.act_func(fpn_out)\n",
        "\n",
        "            output = torch.cat([gat_out,fpn_out],axis=1)\n",
        "        output = self.ffn(output)\n",
        "\n",
        "        if self.is_classif and not self.training:\n",
        "            output = self.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def get_atts_out():\n",
        "    return atts_out\n",
        "\n",
        "def FPGNN(args):\n",
        "    if args.dataset_type == 'classification':\n",
        "        is_classif = 1\n",
        "    else:\n",
        "        is_classif = 0\n",
        "    model = FpgnnModel(is_classif,args.gat_scale,args.cuda,args.dropout)\n",
        "    if args.gat_scale == 1:\n",
        "        model.create_gat(args)\n",
        "        model.create_ffn(args)\n",
        "    elif args.gat_scale == 0:\n",
        "        model.create_fpn(args)\n",
        "        model.create_ffn(args)\n",
        "    else:\n",
        "        model.create_gat(args)\n",
        "        model.create_fpn(args)\n",
        "        model.create_scale(args)\n",
        "        model.create_ffn(args)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        if param.dim() == 1:\n",
        "            nn.init.constant_(param, 0)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(param)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KY1_RzNEL1e3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_metrics(losses, val_metrics, test_metrics, epoch_times):\n",
        "    \"\"\"\n",
        "    Plot training loss, validation metric, and test metric over epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - losses: List of training losses for each epoch.\n",
        "    - val_metrics: List of validation metrics for each epoch.\n",
        "    - test_metrics: List of test metrics for each epoch.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(losses) + 1)\n",
        "\n",
        "    # Plotting training loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, losses, label='Training Loss', marker='o')\n",
        "    plt.title('Training Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting validation and test metrics\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, val_metrics, label='Validation Metric', marker='o')\n",
        "    plt.plot(epochs, test_metrics, label='Test Metric', marker='o')\n",
        "    plt.title('Validation and Test Metrics vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting time vs. epoch\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, epoch_times, label='Time per Epoch', marker='o', color='orange')\n",
        "    plt.title('Epoch Time vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# plot_training_metrics(training_losses, validation_metrics, test_metrics)"
      ],
      "metadata": {
        "id": "QU1Fbi7tq2Tr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "from logging import Logger\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "\n",
        "#         model.zero_grad()\n",
        "#         pred = model(smile)\n",
        "#         loss = loss_f(pred,target) * weight * mask\n",
        "#         loss = loss.sum() / mask.sum()\n",
        "#         loss_sum += loss.item()\n",
        "#         data_used += len(smile)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         if isinstance(scheduler, NoamLR):\n",
        "#             scheduler.step()\n",
        "#     if isinstance(scheduler, ExponentialLR):\n",
        "#         scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "def use_fpgnn(dataset_name, task_type):\n",
        "\n",
        "  # import torch_geometric\n",
        "  # from torch_geometric.datasets import TUDataset\n",
        "  # from torch_geometric.loader import DataLoader\n",
        "  # from torch_geometric.nn import GCNConv, GraphMultisetTransformer\n",
        "  import torch\n",
        "  from torch_geometric.datasets import MoleculeNet\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  # import time\n",
        "\n",
        "  import os.path as osp\n",
        "  import time\n",
        "  from tqdm import tqdm\n",
        "\n",
        "  import torch\n",
        "  import torch.nn.functional as F\n",
        "  from torch.nn import Linear\n",
        "\n",
        "  from torch_geometric.datasets import TUDataset\n",
        "  # from torch_geometric.loader import DataLoader\n",
        "  from torch.utils.data import Dataset, DataLoader, random_split\n",
        "  from torch_geometric.nn import GCNConv, GraphMultisetTransformer\n",
        "  import torch_geometric\n",
        "\n",
        "  ### Importing OGB dataset\n",
        "  from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "  from ogb.graphproppred import Evaluator"
      ],
      "metadata": {
        "id": "0BLqoeDOdd7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "825ac11c-6d05-4fe6-9b1e-8d66cf428b56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7efc2c071240>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0% 0/54 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(38986.0391, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2% 1/54 [00:18<16:01, 18.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(29129.2695, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4% 2/54 [00:27<11:11, 12.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(28008.1172, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6% 3/54 [00:34<08:34, 10.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(28653.3867, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7% 4/54 [00:42<07:43,  9.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(32508.0254, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9% 5/54 [00:48<06:36,  8.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(31599.4922, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11% 6/54 [00:57<06:44,  8.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(29035.6289, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13% 7/54 [01:03<06:00,  7.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(31106.1230, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15% 8/54 [01:10<05:50,  7.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19100.7324, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17% 9/54 [01:16<05:17,  7.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(25296.0352, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19% 10/54 [01:24<05:23,  7.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(30262.8164, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20% 11/54 [01:30<05:00,  6.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26945.5586, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22% 12/54 [01:38<04:59,  7.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26426.9766, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24% 13/54 [01:45<04:53,  7.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19625.0547, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26% 14/54 [01:53<04:51,  7.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26630.7070, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28% 15/54 [01:58<04:24,  6.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24543.0742, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30% 16/54 [02:07<04:45,  7.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(23955.5957, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31% 17/54 [02:16<04:46,  7.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24655.8242, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33% 18/54 [02:21<04:16,  7.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22054.6250, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35% 19/54 [02:29<04:13,  7.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(30315.9727, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37% 20/54 [02:35<03:51,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22827.9590, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39% 21/54 [02:43<03:58,  7.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26177.2148, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41% 22/54 [02:49<03:38,  6.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18675.1621, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43% 23/54 [02:57<03:39,  7.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20500.2480, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44% 24/54 [03:02<03:22,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24056.3379, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46% 25/54 [03:10<03:25,  7.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21545.3066, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48% 26/54 [03:17<03:13,  6.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(25352.1914, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50% 27/54 [03:25<03:12,  7.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19305.2949, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52% 28/54 [03:31<02:56,  6.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22151.8320, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54% 29/54 [03:38<02:56,  7.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18105.5938, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56% 30/54 [03:44<02:40,  6.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24655.4453, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57% 31/54 [03:52<02:39,  6.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(28392.4453, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59% 32/54 [03:58<02:29,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(25502.6094, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61% 33/54 [04:06<02:27,  7.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18035.7871, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63% 34/54 [04:12<02:14,  6.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(13722.0166, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65% 35/54 [04:18<02:08,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19546.2422, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67% 36/54 [04:25<01:58,  6.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19468.7363, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69% 37/54 [04:31<01:49,  6.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17567.9570, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70% 38/54 [04:39<01:51,  6.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(23287.5000, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72% 39/54 [04:45<01:40,  6.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(27059.6562, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74% 40/54 [04:52<01:35,  6.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21822.8574, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76% 41/54 [04:58<01:24,  6.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17707.0430, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78% 42/54 [05:05<01:22,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20899.0996, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80% 43/54 [05:11<01:11,  6.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20954.5586, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81% 44/54 [05:19<01:09,  6.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22197.5449, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83% 45/54 [05:25<00:59,  6.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22609.9727, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85% 46/54 [05:32<00:55,  6.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18697.4199, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87% 47/54 [05:39<00:46,  6.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19474.2871, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89% 48/54 [05:46<00:41,  6.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20179.0410, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91% 49/54 [05:52<00:32,  6.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20243.6523, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93% 50/54 [05:59<00:27,  6.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(15717.3545, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94% 51/54 [06:05<00:19,  6.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(14303.3555, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96% 52/54 [06:12<00:13,  6.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26244.5059, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98% 53/54 [06:18<00:06,  6.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(15641.1348, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 54/54 [06:22<00:00,  7.09s/it]\n",
            "100% 7/7 [00:04<00:00,  1.61it/s]\n",
            "100% 7/7 [00:03<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 23399.7763, Val Acc: 0.0000, Test Acc: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0% 0/54 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19658.8359, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2% 1/54 [00:06<05:28,  6.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20589.2832, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4% 2/54 [00:13<06:06,  7.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26901.4824, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6% 3/54 [00:19<05:30,  6.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(13939.0068, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7% 4/54 [00:26<05:41,  6.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(11458.3301, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9% 5/54 [00:32<05:19,  6.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(35717.6562, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11% 6/54 [00:40<05:32,  6.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22585.3242, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13% 7/54 [00:46<05:04,  6.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17685.3008, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15% 8/54 [00:53<05:04,  6.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17119.2344, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17% 9/54 [01:00<05:02,  6.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20122.2969, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19% 10/54 [01:07<05:05,  6.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18296.7031, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20% 11/54 [01:13<04:42,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22786.2637, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22% 12/54 [01:19<04:28,  6.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20921.1992, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24% 13/54 [01:26<04:30,  6.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18981.3203, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26% 14/54 [01:32<04:13,  6.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21097.9707, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28% 15/54 [01:39<04:20,  6.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20934.2969, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30% 16/54 [01:45<04:02,  6.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18465.0059, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31% 17/54 [01:52<04:11,  6.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20813.2129, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33% 18/54 [01:59<03:59,  6.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(14168.5527, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35% 19/54 [02:06<04:01,  6.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21912.3359, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37% 20/54 [02:12<03:42,  6.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17087.6914, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39% 21/54 [02:19<03:44,  6.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(25343.9551, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41% 22/54 [02:25<03:23,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(14224.0156, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43% 23/54 [02:32<03:29,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16016.8535, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44% 24/54 [02:38<03:12,  6.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19732.2051, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46% 25/54 [02:45<03:12,  6.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18189.5371, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48% 26/54 [02:52<03:03,  6.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16166.3477, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50% 27/54 [02:57<02:49,  6.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21880.0547, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52% 28/54 [03:05<02:52,  6.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(14754.8135, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54% 29/54 [03:10<02:38,  6.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(29459.2520, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56% 30/54 [03:18<02:40,  6.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16430.6836, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57% 31/54 [03:23<02:25,  6.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19549.6445, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 59% 32/54 [03:31<02:27,  6.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20486.0527, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61% 33/54 [03:37<02:14,  6.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(12361., grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63% 34/54 [03:44<02:16,  6.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(15371.0078, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65% 35/54 [03:50<02:02,  6.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20153.8906, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67% 36/54 [03:57<01:59,  6.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24302.1250, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69% 37/54 [04:03<01:48,  6.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19336.3574, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70% 38/54 [04:09<01:40,  6.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16094.1602, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72% 39/54 [04:16<01:37,  6.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20050.7324, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74% 40/54 [04:22<01:27,  6.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17895.9141, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76% 41/54 [04:29<01:26,  6.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(24787.8711, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78% 42/54 [04:35<01:18,  6.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20216.7812, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80% 43/54 [04:43<01:15,  6.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17123.6484, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81% 44/54 [04:49<01:05,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19558.5664, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83% 45/54 [04:57<01:01,  6.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16514.8887, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85% 46/54 [05:02<00:51,  6.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17652.1562, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87% 47/54 [05:09<00:46,  6.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(15454.1396, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89% 48/54 [05:14<00:37,  6.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21354.7539, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91% 49/54 [05:21<00:31,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(22313.2930, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93% 50/54 [05:29<00:26,  6.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(23530.0059, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94% 51/54 [05:34<00:19,  6.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21369.4512, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96% 52/54 [05:41<00:13,  6.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(26727.9004, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98% 53/54 [05:47<00:06,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20855.8828, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 54/54 [05:53<00:00,  6.55s/it]\n",
            "100% 7/7 [00:02<00:00,  2.52it/s]\n",
            "100% 7/7 [00:02<00:00,  2.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 19744.3576, Val Acc: 0.0000, Test Acc: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0% 0/54 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19794.4551, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2% 1/54 [00:07<06:43,  7.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17580.1523, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4% 2/54 [00:13<05:33,  6.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21630.1445, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6% 3/54 [00:19<05:34,  6.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(30273.7480, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7% 4/54 [00:26<05:30,  6.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20190.1230, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9% 5/54 [00:34<05:43,  7.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16799.3145, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11% 6/54 [00:40<05:25,  6.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18718.2656, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13% 7/54 [00:46<05:11,  6.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(23577.3184, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15% 8/54 [00:54<05:12,  6.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(14820.0586, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17% 9/54 [00:59<04:48,  6.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19236.8027, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19% 10/54 [01:06<04:50,  6.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(18406.5625, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20% 11/54 [01:12<04:29,  6.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21937.3027, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22% 12/54 [01:19<04:36,  6.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17026.5176, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24% 13/54 [01:25<04:27,  6.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20855.6914, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26% 14/54 [01:33<04:29,  6.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20015.4902, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28% 15/54 [01:38<04:07,  6.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(17681.1621, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30% 16/54 [01:45<04:07,  6.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(19052.7617, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31% 17/54 [01:51<03:55,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16574.2344, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33% 18/54 [01:57<03:49,  6.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16202.9170, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35% 19/54 [02:05<03:50,  6.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(16500.3926, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37% 20/54 [02:10<03:36,  6.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(21154.1113, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39% 21/54 [02:19<03:49,  6.95s/it]Exception ignored in: <function _xla_gc_callback at 0x7efc55dd36d0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 101, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(20683.3086, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41% 22/54 [02:30<03:38,  6.82s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d7ef1ab58ed2>\u001b[0m in \u001b[0;36m<cell line: 203>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Median time per epoch: {times.clone().detach().requires_grad_(True).median():.4f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m \u001b[0muse_fpgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ToxCast'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-d7ef1ab58ed2>\u001b[0m in \u001b[0;36muse_fpgnn\u001b[0;34m(dataset_name, task_type)\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-d7ef1ab58ed2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;31m# loss = F.cross_entropy(out, data.y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# class_indices = torch.argmax(data.y, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_scale\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgat_scale\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, smile)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mmol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mgat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgat_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mols, smiles)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mone_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matom_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matom_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0matom_start\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0matom_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mgat_atoms_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m             \u001b[0mgat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgat_atoms_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0matom_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mgat_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgat_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mole_out, adj)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mmole_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e600a1ccfdfd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mole_out, adj)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0matom_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmole_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matom_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     28\u001b[0m     Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fpgnn(dataset_name, task_name):\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.optim as optim\n",
        "  from torch.utils.data import DataLoader\n",
        "  import torch\n",
        "  from torch_geometric.datasets import MoleculeNet\n",
        "  import time\n",
        "  from torch.utils.data import DataLoader, random_split\n",
        "  from torch_geometric.data import Batch\n",
        "  from tqdm import tqdm\n",
        "  import numpy as np\n",
        "\n",
        "  dataset = MoleculeNet('\\tmp\\mol', dataset_name).shuffle()\n",
        "  ### Importing OGB dataset\n",
        "  from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "  from ogb.graphproppred import Evaluator\n",
        "  # dataset2= = PygGraphPropPredDataset(name=task_name)\n",
        "  evaluator = Evaluator(task_name)\n",
        "  # Replace YourDataset with your actual dataset class\n",
        "  # from your_module import YourDataset\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  args = Namespace(dataset_type=\"classification\", gat_scale=1, cuda=False, dropout=0.25, nhid=128, dropout_gat=0.25, hidden_size=128, nheads=4,\n",
        "                    task_num=617)\n",
        "  model = FPGNN(args).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "  batch_size=128\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = (len(dataset) - train_size) // 2\n",
        "  test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "  # Split the dataset into train, validation, and test sets\n",
        "  train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "  def collate(data_list):\n",
        "      \"\"\"\n",
        "      Custom collate function to handle torch_geometric.data.data.Data instances.\n",
        "      Converts a list of Data instances to a Batch.\n",
        "      \"\"\"\n",
        "      return Batch.from_data_list(data_list)\n",
        "\n",
        "\n",
        "  # Create data loaders with the custom collate function\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "  clf_criterion = torch.nn.BCEWithLogitsLoss(torch.tensor(10))\n",
        "  reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(model, train_loader, criterion, optimizer, device):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      epoch_iter = tqdm(train_loader, ncols=0)\n",
        "      for step, data in enumerate(epoch_iter):\n",
        "          data = data.to(device)\n",
        "          smiles, labels = data.smiles, data.y\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          output = model(smiles)\n",
        "\n",
        "          # Compute the loss\n",
        "          is_labeled = ~torch.isnan(data.y) # Mask NaNs\n",
        "          loss = criterion(output[is_labeled], data.y[is_labeled])\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss / len(train_loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(model, test_loader, criterion, device):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      total_correct = 0\n",
        "      epoch_iter = tqdm(test_loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          smiles, labels = batch.smiles, batch.y\n",
        "\n",
        "          out = model(batch.smiles)\n",
        "          output = out.cpu().numpy()\n",
        "          print(out)\n",
        "          print(batch.y)\n",
        "          y_true.append(batch.y.view(out.shape).detach().cpu())\n",
        "          y_pred.append(out.detach().cpu())\n",
        "          total_correct += int((torch.argmax(out.to(float), dim=1) == torch.argmax(batch.y.to(float), dim=1)).sum())\n",
        "      y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "      y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "      input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "      return evaluator.eval(input_dict)\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 6):\n",
        "      start = time.time()\n",
        "      train_loss = train(model, train_loader, clf_criterion, optimizer, device)\n",
        "      val_acc = test(model, val_loader, clf_criterion, device)\n",
        "      test_acc = test(model, test_loader, clf_criterion, device)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc}, Test Acc: {test_acc}')\n",
        "      times.append(time.time() - start)\n",
        "\n",
        "run_fpgnn('ToxCast', 'ogbg-moltoxcast')\n"
      ],
      "metadata": {
        "id": "qRJ1N1BKAU_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "dbee6f20-a9fc-4cad-b330-53600128daa9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/clintox.csv.gz\n",
            "Extracting \tmp\\mol/clintox/raw/clintox.csv.gz\n",
            "Processing...\n",
            "Done!\n",
            "  0% 0/10 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-42cedec3e170>\u001b[0m in \u001b[0;36m<cell line: 179>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# run_fpgnn('ToxCast', 'ogbg-moltoxcast')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m \u001b[0mrun_fpgnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ClinTox'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ogbg-molclintox'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-42cedec3e170>\u001b[0m in \u001b[0;36mrun_fpgnn\u001b[0;34m(dataset_name, task_name)\u001b[0m\n\u001b[1;32m    162\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m       \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m       \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m       \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-42cedec3e170>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    103\u001b[0m           \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m           \u001b[0mis_labeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Mask NaNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_labeled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_labeled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m           \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [128, 2] at index 1 does not match the shape of the indexed tensor [128, 617] at index 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_fpgnn(dataset_name, task_name):\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.optim as optim\n",
        "  from torch.utils.data import DataLoader\n",
        "  import torch\n",
        "  from torch_geometric.datasets import MoleculeNet\n",
        "  import time\n",
        "  from torch.utils.data import DataLoader, random_split\n",
        "  from torch_geometric.data import Batch\n",
        "  from tqdm import tqdm\n",
        "  import numpy as np\n",
        "\n",
        "  dataset = MoleculeNet('\\tmp\\mol', dataset_name).shuffle()\n",
        "  ### Importing OGB dataset\n",
        "  from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "  from ogb.graphproppred import Evaluator\n",
        "  evaluator = Evaluator(task_name)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  args = Namespace(dataset_type=\"classification\", gat_scale=1, cuda=False, dropout=0.25, nhid=128, dropout_gat=0.25, hidden_size=128, nheads=4,\n",
        "                    task_num=2)\n",
        "  model = FPGNN(args).to(device)\n",
        "  # print(model)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "  batch_size=128\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = (len(dataset) - train_size) // 2\n",
        "  test_size = len(dataset) - train_size - val_size\n",
        "  # Split the dataset into train, validation, and test sets\n",
        "  train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "  def collate(data_list):\n",
        "      \"\"\"\n",
        "      Custom collate function to handle torch_geometric.data.data.Data instances.\n",
        "      Converts a list of Data instances to a Batch.\n",
        "      \"\"\"\n",
        "      return Batch.from_data_list(data_list)\n",
        "\n",
        "\n",
        "  # Create data loaders with the custom collate function\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "  clf_criterion = torch.nn.BCEWithLogitsLoss(torch.tensor(10))\n",
        "  reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(model, train_loader, criterion, optimizer, device):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      epoch_iter = tqdm(train_loader, ncols=0)\n",
        "      for step, data in enumerate(epoch_iter):\n",
        "          data = data.to(device)\n",
        "          smiles, labels = data.smiles, data.y\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          output = model(smiles)\n",
        "\n",
        "          # Compute the loss\n",
        "          is_labeled = ~torch.isnan(data.y) # Mask NaNs\n",
        "          loss = criterion(output[is_labeled], data.y[is_labeled])\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss / len(train_loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(model, test_loader, criterion, device):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      total_correct = 0\n",
        "      epoch_iter = tqdm(test_loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          smiles, labels = batch.smiles, batch.y\n",
        "\n",
        "          out = model(batch.smiles)\n",
        "          output = out.cpu().numpy()\n",
        "\n",
        "          y_true.append(batch.y.view(out.shape).detach().cpu())\n",
        "          y_pred.append(out.detach().cpu())\n",
        "\n",
        "          total_correct += int((torch.argmax(out.to(float), dim=1) == torch.argmax(batch.y.to(float), dim=1)).sum())\n",
        "      y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "      y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "      input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "      return evaluator.eval(input_dict)\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 6):\n",
        "      start = time.time()\n",
        "      train_loss = train(model, train_loader, clf_criterion, optimizer, device)\n",
        "      val_acc = test(model, val_loader, clf_criterion, device)\n",
        "      test_acc = test(model, test_loader, clf_criterion, device)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc}, Test Acc: {test_acc}')\n",
        "      times.append(time.time() - start)\n",
        "\n",
        "run_fpgnn('ClinTox', 'ogbg-molclintox')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B0mxmciQ7_n",
        "outputId": "64efd412-fe6f-4885-be3b-b78b6ec0b9b9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 10/10 [01:12<00:00,  7.25s/it]\n",
            "100% 2/2 [00:00<00:00,  2.47it/s]\n",
            "100% 2/2 [00:00<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 9.0872, Val Acc: {'rocauc': 0.6692723286159803}, Test Acc: {'rocauc': 0.6616209014019232}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 10/10 [01:13<00:00,  7.37s/it]\n",
            "100% 2/2 [00:00<00:00,  2.48it/s]\n",
            "100% 2/2 [00:00<00:00,  2.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 002, Loss: 4.6899, Val Acc: {'rocauc': 0.673562558403981}, Test Acc: {'rocauc': 0.7107316649287452}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 10/10 [01:11<00:00,  7.16s/it]\n",
            "100% 2/2 [00:00<00:00,  2.65it/s]\n",
            "100% 2/2 [00:00<00:00,  2.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 003, Loss: 3.3969, Val Acc: {'rocauc': 0.6601078292295741}, Test Acc: {'rocauc': 0.7325396825396826}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 10/10 [01:11<00:00,  7.14s/it]\n",
            "100% 2/2 [00:00<00:00,  2.64it/s]\n",
            "100% 2/2 [00:00<00:00,  2.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 004, Loss: 3.0015, Val Acc: {'rocauc': 0.6922229821776382}, Test Acc: {'rocauc': 0.6949339589850538}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% 10/10 [01:11<00:00,  7.14s/it]\n",
            "100% 2/2 [00:00<00:00,  2.11it/s]\n",
            "100% 2/2 [00:00<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 005, Loss: 2.7973, Val Acc: {'rocauc': 0.6991033933416646}, Test Acc: {'rocauc': 0.7272216429150736}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}