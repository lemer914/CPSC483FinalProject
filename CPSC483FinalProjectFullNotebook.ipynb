{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orBsadPzi_Fg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# import pkg_resources\n",
        "# pkg_resources.require('Torch==2.0.1')\n",
        "!pip install torch==2.0.1\n",
        "import torch\n",
        "print('Using torch', torch.__version__)\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install ogb\n",
        "!pip install rdkit-pypi\n",
        "\n",
        "\n",
        "from torch_geometric.datasets import MoleculeNet\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib\n",
        "import torch_geometric\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "\n",
        "from torch_geometric.nn.inits import uniform\n",
        "\n",
        "from torch_scatter import scatter_mean\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch_geometric.nn import GCNConv, GraphMultisetTransformer\n",
        "\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "\n",
        "from ogb.graphproppred.dataset_pyg import PygGraphPropPredDataset\n",
        "from ogb.graphproppred import Evaluator\n",
        "\n",
        "from ogb.graphproppred.mol_encoder import BondEncoder\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, MACCSkeys\n",
        "import csv\n",
        "import re\n",
        "\n",
        "from torch.nn import Linear\n",
        "from tqdm import tqdm\n",
        "\n",
        "from argparse import Namespace\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool as gmp, global_add_pool as gsp\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "\n",
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from logging import Logger\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "\n",
        "# set seed\n",
        "seed_value = 37\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "class MAB(nn.Module):\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, cluster=False, conv=None):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "\n",
        "        self.fc_k, self.fc_v = self.get_fc_kv(dim_K, dim_V, conv)\n",
        "\n",
        "        if ln:\n",
        "            self.ln0 = nn.LayerNorm(dim_V)\n",
        "            self.ln1 = nn.LayerNorm(dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "\n",
        "        self.softmax_dim = 2\n",
        "        if cluster == True:\n",
        "            self.softmax_dim = 1\n",
        "\n",
        "    def forward(self, Q, K, attention_mask=None, graph=None, return_attn=False):\n",
        "        Q = self.fc_q(Q)\n",
        "\n",
        "        # Adj: Exist (graph is not None), or Identity (else)\n",
        "        if graph is not None:\n",
        "\n",
        "            (x, edge_index, batch) = graph\n",
        "\n",
        "            K, V = self.fc_k(x, edge_index), self.fc_v(x, edge_index)\n",
        "\n",
        "            K, _ = to_dense_batch(K, batch)\n",
        "            V, _ = to_dense_batch(V, batch)\n",
        "\n",
        "        else:\n",
        "\n",
        "            K, V = self.fc_k(K), self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
        "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
        "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = torch.cat([attention_mask for _ in range(self.num_heads)], 0)\n",
        "            attention_score = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
        "            A = torch.softmax(attention_mask + attention_score, self.softmax_dim)\n",
        "        else:\n",
        "            A = torch.softmax(Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V), self.softmax_dim)\n",
        "\n",
        "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
        "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
        "        O = O + F.relu(self.fc_o(O))\n",
        "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
        "        if return_attn:\n",
        "            return O, A\n",
        "        else:\n",
        "            return O\n",
        "\n",
        "    def get_fc_kv(self, dim_K, dim_V, conv):\n",
        "\n",
        "        if conv == 'GCN':\n",
        "\n",
        "            fc_k = GCNConv(dim_K, dim_V)\n",
        "            fc_v = GCNConv(dim_K, dim_V)\n",
        "\n",
        "        elif conv == 'GIN':\n",
        "\n",
        "            fc_k = GINConv(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(dim_K, dim_K),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(dim_K, dim_V),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(dim_V),\n",
        "            ), train_eps=False)\n",
        "\n",
        "            fc_v = GINConv(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(dim_K, dim_K),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(dim_K, dim_V),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(dim_V),\n",
        "            ), train_eps=False)\n",
        "\n",
        "        else:\n",
        "\n",
        "            fc_k = nn.Linear(dim_K, dim_V)\n",
        "            fc_v = nn.Linear(dim_K, dim_V)\n",
        "\n",
        "        return fc_k, fc_v\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, ln=False, cluster=False, mab_conv=None):\n",
        "        super(SAB, self).__init__()\n",
        "\n",
        "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None):\n",
        "        return self.mab(X, X, attention_mask, graph)\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, cluster=False, mab_conv=None):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "\n",
        "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None):\n",
        "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, attention_mask, graph)\n",
        "        return self.mab1(X, H)\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_seeds, ln=False, cluster=False, mab_conv=None):\n",
        "        super(PMA, self).__init__()\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None, return_attn=False):\n",
        "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, attention_mask, graph, return_attn)\n",
        "\n",
        "### GCN convolution along the graph structure\n",
        "class GCNConv_for_OGB(MessagePassing):\n",
        "    def __init__(self, emb_dim):\n",
        "        super(GCNConv_for_OGB, self).__init__(aggr='add')\n",
        "\n",
        "        self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
        "        self.root_emb = torch.nn.Embedding(1, emb_dim)\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.linear(x)\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        deg = degree(row, x.size(0), dtype = x.dtype) + 1\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_embedding, norm=norm) + F.relu(x + self.root_emb.weight) * 1./deg.view(-1,1)\n",
        "\n",
        "    def message(self, x_j, edge_attr, norm):\n",
        "        return norm.view(-1, 1) * F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "### GIN convolution along the graph structure\n",
        "class GINConv_for_OGB(MessagePassing):\n",
        "    def __init__(self, emb_dim):\n",
        "        super(GINConv_for_OGB, self).__init__(aggr = \"add\")\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
        "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "        out = self.mlp((1 + self.eps) * x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "        return F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "class GraphRepresentation(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphRepresentation, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.num_features = args.num_features\n",
        "        self.nhid = args.num_hidden\n",
        "        self.num_classes = args.num_classes\n",
        "        self.pooling_ratio = args.pooling_ratio\n",
        "        self.dropout_ratio = args.dropout\n",
        "\n",
        "    def get_convs(self):\n",
        "\n",
        "        convs = nn.ModuleList()\n",
        "\n",
        "        _input_dim = self.num_features\n",
        "        _output_dim = self.nhid\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            if self.args.conv == 'GCN':\n",
        "\n",
        "                conv = GCNConv(_input_dim, _output_dim)\n",
        "\n",
        "            elif self.args.conv == 'GIN':\n",
        "\n",
        "                conv = GINConv(\n",
        "                    nn.Sequential(\n",
        "                        nn.Linear(_input_dim, _output_dim),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(_output_dim, _output_dim),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm1d(_output_dim),\n",
        "                ), train_eps=False)\n",
        "\n",
        "            convs.append(conv)\n",
        "\n",
        "            _input_dim = _output_dim\n",
        "            _output_dim = _output_dim\n",
        "\n",
        "        return convs\n",
        "\n",
        "    def get_pools(self):\n",
        "\n",
        "        pools = nn.ModuleList([gap])\n",
        "\n",
        "        return pools\n",
        "\n",
        "    def get_classifier(self):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.nhid, self.nhid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=self.dropout_ratio),\n",
        "            nn.Linear(self.nhid, self.nhid//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=self.dropout_ratio),\n",
        "            nn.Linear(self.nhid//2, self.num_classes)\n",
        "        )\n",
        "\n",
        "class GraphMultisetTransformer(GraphRepresentation):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphMultisetTransformer, self).__init__(args)\n",
        "\n",
        "        self.ln = args.ln\n",
        "        self.num_heads = args.num_heads\n",
        "        self.cluster = args.cluster\n",
        "\n",
        "        self.model_sequence = args.model_string.split('-')\n",
        "\n",
        "        self.convs = self.get_convs()\n",
        "        self.pools = self.get_pools()\n",
        "        self.classifier = self.get_classifier()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # For Graph Convolution Network\n",
        "        xs = []\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            x = F.relu(self.convs[_](x, edge_index))\n",
        "            xs.append(x)\n",
        "\n",
        "        # For jumping knowledge scheme\n",
        "        x = torch.cat(xs, dim=1)\n",
        "\n",
        "        # For Graph Multiset Transformer\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if _index == 0:\n",
        "\n",
        "                batch_x, mask = to_dense_batch(x, batch)\n",
        "\n",
        "                extended_attention_mask = mask.unsqueeze(1)\n",
        "                extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask, graph=(x, edge_index, batch))\n",
        "\n",
        "            else:\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask)\n",
        "\n",
        "            extended_attention_mask = None\n",
        "\n",
        "        batch_x = self.pools[len(self.model_sequence)](batch_x)\n",
        "        x = batch_x.squeeze(1)\n",
        "\n",
        "        # For Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "    def get_pools(self, _input_dim=None, reconstruction=False):\n",
        "\n",
        "        pools = nn.ModuleList()\n",
        "\n",
        "        _input_dim = self.nhid * self.args.num_convs if _input_dim is None else _input_dim\n",
        "        _output_dim = self.nhid\n",
        "        _num_nodes = ceil(self.pooling_ratio * self.args.avg_num_nodes)\n",
        "\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if (_index == len(self.model_sequence) - 1) and (reconstruction == False):\n",
        "\n",
        "                _num_nodes = 1\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                pools.append(\n",
        "                    PMA(_input_dim, self.num_heads, _num_nodes, ln=self.ln, cluster=self.cluster, mab_conv=self.args.mab_conv)\n",
        "                )\n",
        "\n",
        "                _num_nodes = ceil(self.pooling_ratio * _num_nodes)\n",
        "\n",
        "            elif _model_str == 'GMPool_I':\n",
        "\n",
        "                pools.append(\n",
        "                    PMA(_input_dim, self.num_heads, _num_nodes, ln=self.ln, cluster=self.cluster, mab_conv=None)\n",
        "                )\n",
        "\n",
        "                _num_nodes = ceil(self.pooling_ratio * _num_nodes)\n",
        "\n",
        "            elif _model_str == 'SelfAtt':\n",
        "\n",
        "                pools.append(\n",
        "                    SAB(_input_dim, _output_dim, self.num_heads, ln=self.ln, cluster=self.cluster)\n",
        "                )\n",
        "\n",
        "                _input_dim = _output_dim\n",
        "                _output_dim = _output_dim\n",
        "\n",
        "            else:\n",
        "\n",
        "                raise ValueError(\"Model Name in Model String <{}> is Unknown\".format(_model_str))\n",
        "\n",
        "        pools.append(nn.Linear(_input_dim, self.nhid))\n",
        "\n",
        "        return pools\n",
        "\n",
        "class GraphMultisetTransformer_for_OGB(GraphMultisetTransformer):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphMultisetTransformer_for_OGB, self).__init__(args)\n",
        "\n",
        "        self.atom_encoder = AtomEncoder(self.nhid)\n",
        "        self.convs = self.get_convs()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        x = self.atom_encoder(x)\n",
        "\n",
        "        # For Graph Convolution Network\n",
        "        xs = []\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            x = F.relu(self.convs[_](x, edge_index, edge_attr))\n",
        "            xs.append(x)\n",
        "\n",
        "        # For jumping knowledge scheme\n",
        "        x = torch.cat(xs, dim=1)\n",
        "\n",
        "        # For Graph Multiset Transformer\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if _index == 0:\n",
        "\n",
        "                batch_x, mask = to_dense_batch(x, batch)\n",
        "\n",
        "                extended_attention_mask = mask.unsqueeze(1)\n",
        "                extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask, graph=(x, edge_index, batch))\n",
        "\n",
        "            else:\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask)\n",
        "\n",
        "            extended_attention_mask = None\n",
        "\n",
        "        batch_x = self.pools[len(self.model_sequence)](batch_x)\n",
        "        x = batch_x.squeeze(1)\n",
        "\n",
        "        # For Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_convs(self):\n",
        "\n",
        "        convs = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            if self.args.conv == 'GCN':\n",
        "\n",
        "                conv = GCNConv_for_OGB(self.nhid)\n",
        "\n",
        "            elif self.args.conv == 'GIN':\n",
        "\n",
        "                conv = GINConv_for_OGB(self.nhid)\n",
        "\n",
        "            convs.append(conv)\n",
        "\n",
        "        return convs\n",
        "\n",
        "def plot_training_metrics(losses, val_metrics, test_metrics, epoch_times):\n",
        "    \"\"\"\n",
        "    Plot training loss, validation metric, and test metric over epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - losses: List of training losses for each epoch.\n",
        "    - val_metrics: List of validation metrics for each epoch.\n",
        "    - test_metrics: List of test metrics for each epoch.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(losses) + 1)\n",
        "\n",
        "    # Plotting training loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, losses, label='Training Loss', marker='o')\n",
        "    plt.title('Training Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting validation and test metrics\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, val_metrics, label='Validation Metric', marker='o')\n",
        "    plt.plot(epochs, test_metrics, label='Test Metric', marker='o')\n",
        "    plt.title('Validation and Test Metrics vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plotting time vs. epoch\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, epoch_times, label='Time per Epoch', marker='o', color='orange')\n",
        "    plt.title('Time vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# plot_training_metrics(training_losses, validation_metrics, test_metrics)\n",
        "\n",
        "\n",
        "\n",
        "def tud_trans(dataset_name, k, h):\n",
        "  path = 'tmp\\tud'\n",
        "  dataset = TUDataset(path, name=dataset_name).shuffle()\n",
        "\n",
        "  n = (len(dataset) + 9) // 10\n",
        "  train_dataset = dataset[2 * n:]\n",
        "  val_dataset = dataset[n:2 * n]\n",
        "  test_dataset = dataset[:n]\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "\n",
        "  class Net(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super().__init__()\n",
        "\n",
        "          self.conv1 = GCNConv(dataset.num_features, 32)\n",
        "          self.conv2 = GCNConv(32, 32)\n",
        "          self.conv3 = GCNConv(32, 32)\n",
        "\n",
        "          self.pool = GraphMultisetTransformer(96, k=k, heads=h)\n",
        "\n",
        "          self.lin1 = Linear(96, 16)\n",
        "          self.lin2 = Linear(16, dataset.num_classes)\n",
        "\n",
        "      def forward(self, x0, edge_index, batch):\n",
        "          x1 = self.conv1(x0, edge_index).relu()\n",
        "          x2 = self.conv2(x1, edge_index).relu()\n",
        "          x3 = self.conv3(x2, edge_index).relu()\n",
        "          x = torch.cat([x1, x2, x3], dim=-1)\n",
        "\n",
        "          x = self.pool(x, batch)\n",
        "\n",
        "          x = self.lin1(x).relu()\n",
        "          x = F.dropout(x, p=0.5, training=self.training)\n",
        "          x = self.lin2(x)\n",
        "\n",
        "          return x\n",
        "\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = Net().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "  def train():\n",
        "      model.train()\n",
        "\n",
        "      total_loss = 0\n",
        "      for data in train_loader:\n",
        "          data = data.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = F.cross_entropy(out, data.y)\n",
        "          loss.backward()\n",
        "          total_loss += data.num_graphs * float(loss)\n",
        "          optimizer.step()\n",
        "      return total_loss / len(train_dataset)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(loader):\n",
        "      model.eval()\n",
        "\n",
        "      total_correct = 0\n",
        "      for data in loader:\n",
        "          data = data.to(device)\n",
        "          out = model(data.x, data.edge_index, data.batch)\n",
        "          total_correct += int((out.argmax(dim=-1) == data.y).sum())\n",
        "      return total_correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 101):\n",
        "      start = time.time()\n",
        "      train_loss = train()\n",
        "      val_acc = test(val_loader)\n",
        "      test_acc = test(test_loader)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "      times.append(time.time() - start)\n",
        "  print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "\n",
        "  plot_training_metrics(training_losses, validation_metrics, test_metrics, times)\n",
        "  print(f'Best Validation Metric: {max(validation_metrics)}')\n",
        "  print(f'Test Metric: {test_metrics[np.argmax(validation_metrics)]}')\n",
        "\n",
        "tud_trans('PROTEINS', k=10, h=4)\n",
        "tud_trans('DD', k=8, h=2)\n",
        "\n",
        "# baseline GCNConv comparison\n",
        "def conv_TUD(dataset_name):\n",
        "\n",
        "  path = '\\tmp\\tud'\n",
        "  dataset = TUDataset(path, dataset_name).shuffle()\n",
        "\n",
        "  num_classes = dataset.num_classes\n",
        "  num_features = dataset.num_features\n",
        "  hidden_channels = 32\n",
        "  task_type='classification'\n",
        "  # print(num_classes)\n",
        "\n",
        "  def collate(data_list):\n",
        "    batch = torch_geometric.data.Batch.from_data_list(data_list)\n",
        "    return batch\n",
        "\n",
        "  total_size = len(dataset)\n",
        "  train_size = int(0.8 * total_size)\n",
        "  test_size = (total_size - train_size) // 2\n",
        "  val_size = total_size - train_size - test_size\n",
        "\n",
        "  # Use random_split to split the dataset into train, test, and validation sets\n",
        "  train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=128, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate)\n",
        "\n",
        "  class NN(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super().__init__()\n",
        "\n",
        "          self.conv1 = GCNConv(in_channels=num_features, out_channels=hidden_channels, normalize=True)\n",
        "          self.conv2 = GCNConv(in_channels=hidden_channels, out_channels=num_classes, normalize=True)\n",
        "          self.relu = torch.nn.ReLU()\n",
        "\n",
        "          self.lin1 = Linear(32, 16)\n",
        "          self.lin2 = Linear(16, num_classes)\n",
        "\n",
        "      def forward(self, x0, edge_index, batch):\n",
        "          x = self.conv1(x0, edge_index)\n",
        "          x = self.relu(x)\n",
        "          x = self.conv2(x, edge_index)\n",
        "          # global mean pool instead of GMT, along with other adjustments\n",
        "          output = global_mean_pool(x, batch)\n",
        "          # output = output.detach().clone().requires_grad_(True)\n",
        "          # x = self.lin1(x)\n",
        "          # output = self.lin2(x)\n",
        "\n",
        "          return output\n",
        "\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = NN().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "  def train(loader):\n",
        "      model.train()\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      epoch_iter = tqdm(loader, ncols=130)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          out = model(batch.x, batch.edge_index, batch.batch)\n",
        "          is_labeled = ~torch.isnan(batch.y) # Mask NaNs\n",
        "          loss = F.cross_entropy(out[is_labeled], batch.y[is_labeled])\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients if needed, uncomment\n",
        "          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          total_loss += batch.num_graphs * float(loss)\n",
        "          optimizer.step()\n",
        "      return total_loss / len(train_dataset)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(loader):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      total_same = 0\n",
        "      epoch_iter = tqdm(loader, ncols=130)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          out = model(batch.x, batch.edge_index, batch.batch)\n",
        "          total_same += int((out.argmax(dim=-1) == batch.y).sum())\n",
        "      return total_same / len(loader.dataset)\n",
        "\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 201):\n",
        "      start = time.time()\n",
        "      train_loss = train(train_loader)\n",
        "      val_acc = test(val_loader)\n",
        "      test_acc = test(test_loader)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
        "      times.append(time.time() - start)\n",
        "  print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "  plot_training_metrics(training_losses, validation_metrics, test_metrics, times)\n",
        "  print(f'Best Validation Metric: {max(validation_metrics)}')\n",
        "  print(f'Test Metric: {test_metrics[np.argmax(validation_metrics)]}')\n",
        "\n",
        "conv_TUD('DD')\n",
        "conv_TUD('PROTEINS')\n",
        "\n",
        "# set seed\n",
        "seed_value = 108\n",
        "torch.manual_seed(seed_value)\n",
        "\n",
        "class MAB(nn.Module):\n",
        "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False, cluster=False, conv=None):\n",
        "        super(MAB, self).__init__()\n",
        "        self.dim_V = dim_V\n",
        "        self.num_heads = num_heads\n",
        "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
        "\n",
        "        self.fc_k, self.fc_v = self.get_fc_kv(dim_K, dim_V, conv)\n",
        "\n",
        "        if ln:\n",
        "            self.ln0 = nn.LayerNorm(dim_V)\n",
        "            self.ln1 = nn.LayerNorm(dim_V)\n",
        "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
        "\n",
        "        self.softmax_dim = 2\n",
        "        if cluster == True:\n",
        "            self.softmax_dim = 1\n",
        "\n",
        "    def forward(self, Q, K, attention_mask=None, graph=None, return_attn=False):\n",
        "        Q = self.fc_q(Q)\n",
        "\n",
        "        # Adj: Exist (graph is not None), or Identity (else)\n",
        "        if graph is not None:\n",
        "\n",
        "            (x, edge_index, batch) = graph\n",
        "\n",
        "            K, V = self.fc_k(x, edge_index), self.fc_v(x, edge_index)\n",
        "\n",
        "            K, _ = to_dense_batch(K, batch)\n",
        "            V, _ = to_dense_batch(V, batch)\n",
        "\n",
        "        else:\n",
        "\n",
        "            K, V = self.fc_k(K), self.fc_v(K)\n",
        "\n",
        "        dim_split = self.dim_V // self.num_heads\n",
        "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
        "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
        "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = torch.cat([attention_mask for _ in range(self.num_heads)], 0)\n",
        "            attention_score = Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V)\n",
        "            A = torch.softmax(attention_mask + attention_score, self.softmax_dim)\n",
        "        else:\n",
        "            A = torch.softmax(Q_.bmm(K_.transpose(1,2))/math.sqrt(self.dim_V), self.softmax_dim)\n",
        "\n",
        "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
        "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
        "        O = O + F.relu(self.fc_o(O))\n",
        "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
        "        if return_attn:\n",
        "            return O, A\n",
        "        else:\n",
        "            return O\n",
        "\n",
        "    def get_fc_kv(self, dim_K, dim_V, conv):\n",
        "\n",
        "        if conv == 'GCN':\n",
        "\n",
        "            fc_k = GCNConv(dim_K, dim_V)\n",
        "            fc_v = GCNConv(dim_K, dim_V)\n",
        "\n",
        "        elif conv == 'GIN':\n",
        "\n",
        "            fc_k = GINConv(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(dim_K, dim_K),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(dim_K, dim_V),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(dim_V),\n",
        "            ), train_eps=False)\n",
        "\n",
        "            fc_v = GINConv(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(dim_K, dim_K),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(dim_K, dim_V),\n",
        "                    nn.ReLU(),\n",
        "                    nn.BatchNorm1d(dim_V),\n",
        "            ), train_eps=False)\n",
        "\n",
        "        else:\n",
        "\n",
        "            fc_k = nn.Linear(dim_K, dim_V)\n",
        "            fc_v = nn.Linear(dim_K, dim_V)\n",
        "\n",
        "        return fc_k, fc_v\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, ln=False, cluster=False, mab_conv=None):\n",
        "        super(SAB, self).__init__()\n",
        "\n",
        "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None):\n",
        "        return self.mab(X, X, attention_mask, graph)\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False, cluster=False, mab_conv=None):\n",
        "        super(ISAB, self).__init__()\n",
        "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
        "        nn.init.xavier_uniform_(self.I)\n",
        "\n",
        "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None):\n",
        "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X, attention_mask, graph)\n",
        "        return self.mab1(X, H)\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_seeds, ln=False, cluster=False, mab_conv=None):\n",
        "        super(PMA, self).__init__()\n",
        "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
        "        nn.init.xavier_uniform_(self.S)\n",
        "\n",
        "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln, cluster=cluster, conv=mab_conv)\n",
        "\n",
        "    def forward(self, X, attention_mask=None, graph=None, return_attn=False):\n",
        "        return self.mab(self.S.repeat(X.size(0), 1, 1), X, attention_mask, graph, return_attn)\n",
        "\n",
        "### GCN convolution along the graph structure\n",
        "class GCNConv_for_OGB(MessagePassing):\n",
        "    def __init__(self, emb_dim):\n",
        "        super(GCNConv_for_OGB, self).__init__(aggr='add')\n",
        "\n",
        "        self.linear = torch.nn.Linear(emb_dim, emb_dim)\n",
        "        self.root_emb = torch.nn.Embedding(1, emb_dim)\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        x = self.linear(x)\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "\n",
        "        row, col = edge_index\n",
        "\n",
        "        deg = degree(row, x.size(0), dtype = x.dtype) + 1\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_embedding, norm=norm) + F.relu(x + self.root_emb.weight) * 1./deg.view(-1,1)\n",
        "\n",
        "    def message(self, x_j, edge_attr, norm):\n",
        "        return norm.view(-1, 1) * F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "### GIN convolution along the graph structure\n",
        "class GINConv_for_OGB(MessagePassing):\n",
        "    def __init__(self, emb_dim):\n",
        "        super(GINConv_for_OGB, self).__init__(aggr = \"add\")\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
        "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "        self.bond_encoder = BondEncoder(emb_dim = emb_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        edge_embedding = self.bond_encoder(edge_attr)\n",
        "        out = self.mlp((1 + self.eps) * x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "        return F.relu(x_j + edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "class GraphRepresentation(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphRepresentation, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.num_features = args.num_features\n",
        "        self.nhid = args.num_hidden\n",
        "        self.num_classes = args.num_classes\n",
        "        self.pooling_ratio = args.pooling_ratio\n",
        "        self.dropout_ratio = args.dropout\n",
        "\n",
        "    def get_convs(self):\n",
        "\n",
        "        convs = nn.ModuleList()\n",
        "\n",
        "        _input_dim = self.num_features\n",
        "        _output_dim = self.nhid\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            if self.args.conv == 'GCN':\n",
        "\n",
        "                conv = GCNConv(_input_dim, _output_dim)\n",
        "\n",
        "            elif self.args.conv == 'GIN':\n",
        "\n",
        "                conv = GINConv(\n",
        "                    nn.Sequential(\n",
        "                        nn.Linear(_input_dim, _output_dim),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(_output_dim, _output_dim),\n",
        "                        nn.ReLU(),\n",
        "                        nn.BatchNorm1d(_output_dim),\n",
        "                ), train_eps=False)\n",
        "\n",
        "            convs.append(conv)\n",
        "\n",
        "            _input_dim = _output_dim\n",
        "            _output_dim = _output_dim\n",
        "\n",
        "        return convs\n",
        "\n",
        "    def get_pools(self):\n",
        "\n",
        "        pools = nn.ModuleList([gap])\n",
        "\n",
        "        return pools\n",
        "\n",
        "    def get_classifier(self):\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.nhid, self.nhid),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=self.dropout_ratio),\n",
        "            nn.Linear(self.nhid, self.nhid//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=self.dropout_ratio),\n",
        "            nn.Linear(self.nhid//2, self.num_classes)\n",
        "        )\n",
        "\n",
        "class GraphMultisetTransformer(GraphRepresentation):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphMultisetTransformer, self).__init__(args)\n",
        "\n",
        "        self.ln = args.ln\n",
        "        self.num_heads = args.num_heads\n",
        "        self.cluster = args.cluster\n",
        "\n",
        "        self.model_sequence = args.model_string.split('-')\n",
        "\n",
        "        self.convs = self.get_convs()\n",
        "        self.pools = self.get_pools()\n",
        "        self.classifier = self.get_classifier()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # For Graph Convolution Network\n",
        "        xs = []\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            x = F.relu(self.convs[_](x, edge_index))\n",
        "            xs.append(x)\n",
        "\n",
        "        # For jumping knowledge scheme\n",
        "        x = torch.cat(xs, dim=1)\n",
        "\n",
        "        # For Graph Multiset Transformer\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if _index == 0:\n",
        "\n",
        "                batch_x, mask = to_dense_batch(x, batch)\n",
        "\n",
        "                extended_attention_mask = mask.unsqueeze(1)\n",
        "                extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask, graph=(x, edge_index, batch))\n",
        "\n",
        "            else:\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask)\n",
        "\n",
        "            extended_attention_mask = None\n",
        "\n",
        "        batch_x = self.pools[len(self.model_sequence)](batch_x)\n",
        "        x = batch_x.squeeze(1)\n",
        "\n",
        "        # For Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "    def get_pools(self, _input_dim=None, reconstruction=False):\n",
        "\n",
        "        pools = nn.ModuleList()\n",
        "\n",
        "        _input_dim = self.nhid * self.args.num_convs if _input_dim is None else _input_dim\n",
        "        _output_dim = self.nhid\n",
        "        _num_nodes = ceil(self.pooling_ratio * self.args.avg_num_nodes)\n",
        "\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if (_index == len(self.model_sequence) - 1) and (reconstruction == False):\n",
        "\n",
        "                _num_nodes = 1\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                pools.append(\n",
        "                    PMA(_input_dim, self.num_heads, _num_nodes, ln=self.ln, cluster=self.cluster, mab_conv=self.args.mab_conv)\n",
        "                )\n",
        "\n",
        "                _num_nodes = ceil(self.pooling_ratio * _num_nodes)\n",
        "\n",
        "            elif _model_str == 'GMPool_I':\n",
        "\n",
        "                pools.append(\n",
        "                    PMA(_input_dim, self.num_heads, _num_nodes, ln=self.ln, cluster=self.cluster, mab_conv=None)\n",
        "                )\n",
        "\n",
        "                _num_nodes = ceil(self.pooling_ratio * _num_nodes)\n",
        "\n",
        "            elif _model_str == 'SelfAtt':\n",
        "\n",
        "                pools.append(\n",
        "                    SAB(_input_dim, _output_dim, self.num_heads, ln=self.ln, cluster=self.cluster)\n",
        "                )\n",
        "\n",
        "                _input_dim = _output_dim\n",
        "                _output_dim = _output_dim\n",
        "\n",
        "            else:\n",
        "\n",
        "                raise ValueError(\"Model Name in Model String <{}> is Unknown\".format(_model_str))\n",
        "\n",
        "        pools.append(nn.Linear(_input_dim, self.nhid))\n",
        "\n",
        "        return pools\n",
        "\n",
        "class GraphMultisetTransformer_for_OGB(GraphMultisetTransformer):\n",
        "\n",
        "    def __init__(self, args):\n",
        "\n",
        "        super(GraphMultisetTransformer_for_OGB, self).__init__(args)\n",
        "\n",
        "        self.atom_encoder = AtomEncoder(self.nhid)\n",
        "        self.convs = self.get_convs()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        x = self.atom_encoder(x)\n",
        "\n",
        "        # For Graph Convolution Network\n",
        "        xs = []\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            x = F.relu(self.convs[_](x, edge_index, edge_attr))\n",
        "            xs.append(x)\n",
        "\n",
        "        # For jumping knowledge scheme\n",
        "        x = torch.cat(xs, dim=1)\n",
        "\n",
        "        # For Graph Multiset Transformer\n",
        "        for _index, _model_str in enumerate(self.model_sequence):\n",
        "\n",
        "            if _index == 0:\n",
        "\n",
        "                batch_x, mask = to_dense_batch(x, batch)\n",
        "\n",
        "                extended_attention_mask = mask.unsqueeze(1)\n",
        "                extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
        "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
        "\n",
        "            if _model_str == 'GMPool_G':\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask, graph=(x, edge_index, batch))\n",
        "\n",
        "            else:\n",
        "\n",
        "                batch_x = self.pools[_index](batch_x, attention_mask=extended_attention_mask)\n",
        "\n",
        "            extended_attention_mask = None\n",
        "\n",
        "        batch_x = self.pools[len(self.model_sequence)](batch_x)\n",
        "        x = batch_x.squeeze(1)\n",
        "\n",
        "        # For Classification\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_convs(self):\n",
        "\n",
        "        convs = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.args.num_convs):\n",
        "\n",
        "            if self.args.conv == 'GCN':\n",
        "\n",
        "                conv = GCNConv_for_OGB(self.nhid)\n",
        "\n",
        "            elif self.args.conv == 'GIN':\n",
        "\n",
        "                conv = GINConv_for_OGB(self.nhid)\n",
        "\n",
        "            convs.append(conv)\n",
        "\n",
        "        return convs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NNDecoder(torch.nn.Module):\n",
        "    def __init__(self, num_tasks, emb_dim, graph_pooling = \"mean\"):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(emb_dim, num_tasks)\n",
        "\n",
        "    def forward(self, node_rep):\n",
        "      ## (~2-3 lines of code)\n",
        "      output = self.linear(node_rep[:,0,:])\n",
        "      return output\n",
        "\n",
        "# baseline GCNConv comparison\n",
        "def conv_OGB(dataset_name):\n",
        "\n",
        "  path = '\\tmp\\ogb'\n",
        "  # dataset = MoleculeNet(path, dataset_name).shuffle()\n",
        "  ### Importing OGB dataset\n",
        "\n",
        "\n",
        "  dataset = PygGraphPropPredDataset(name=dataset_name)\n",
        "  evaluator = Evaluator(dataset_name)\n",
        "  clf_criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "  num_classes = dataset.num_classes\n",
        "  num_features = dataset.num_features\n",
        "  num_tasks = dataset.num_tasks\n",
        "  hidden_channels = 32\n",
        "  task_type='classification'\n",
        "  # print(num_classes)\n",
        "\n",
        "  def collate(data_list):\n",
        "    batch = torch_geometric.data.Batch.from_data_list(data_list)\n",
        "    return batch\n",
        "\n",
        "  total_size = len(dataset)\n",
        "  train_size = int(0.8 * total_size)\n",
        "  test_size = (total_size - train_size) // 2\n",
        "  val_size = total_size - train_size - test_size\n",
        "\n",
        "  # Use random_split to split the dataset into train, test, and validation sets\n",
        "  train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=128, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=collate)\n",
        "\n",
        "  class NN(torch.nn.Module):\n",
        "      def __init__(self):\n",
        "          super().__init__()\n",
        "\n",
        "          self.conv1 = GCNConv(in_channels=num_features, out_channels=hidden_channels, normalize=True)\n",
        "          # was hidden, num_tasks\n",
        "          self.conv2 = GCNConv(in_channels=hidden_channels, out_channels=hidden_channels, normalize=True)\n",
        "          self.relu = torch.nn.ReLU()\n",
        "\n",
        "          # was 32, 16\n",
        "          self.lin1 = Linear(hidden_channels, num_tasks)\n",
        "          self.lin2 = Linear(16, num_tasks)\n",
        "\n",
        "      def forward(self, x0, edge_index, batch):\n",
        "          x = self.conv1(x0, edge_index)\n",
        "          x = self.relu(x)\n",
        "          x = self.conv2(x, edge_index)\n",
        "          x = self.lin1(x)\n",
        "          # global mean pool instead of GMT, along with other adjustments\n",
        "          output = global_mean_pool(x, batch)\n",
        "\n",
        "          return output\n",
        "\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = NN().to(device)\n",
        "  decoder = NNDecoder(emb_dim = 32, num_tasks = dataset.num_tasks).to(device)\n",
        "  print(dataset.num_tasks)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "  def train(loader):\n",
        "      model.train()\n",
        "\n",
        "      total_loss = 0\n",
        "\n",
        "      epoch_iter = tqdm(loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
        "          is_labeled = ~torch.isnan(batch.y) # Mask NaNs\n",
        "          loss = clf_criterion(out.float()[is_labeled], batch.y.float()[is_labeled])\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients if needed, uncomment\n",
        "          # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          total_loss += batch.num_graphs * float(loss)\n",
        "          optimizer.step()\n",
        "      print('loss returned:', total_loss / len(train_dataset))\n",
        "      return total_loss / len(train_dataset)\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(loader):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      total_same = 0\n",
        "      epoch_iter = tqdm(loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
        "          y_true.append(batch.y.view(out.shape).detach().cpu())\n",
        "          y_pred.append(out.detach().cpu())\n",
        "      y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "      y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "      input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "      return evaluator.eval(input_dict)\n",
        "\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 201):\n",
        "      start = time.time()\n",
        "      train_loss = train(train_loader)\n",
        "      val_acc = test(val_loader)\n",
        "      test_acc = test(test_loader)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc[dataset.eval_metric])\n",
        "      test_metrics.append(test_acc[dataset.eval_metric])\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc}, Test Acc: {test_acc}')\n",
        "      times.append(time.time() - start)\n",
        "  best_val_epoch = np.argmax(np.array(validation_metrics))\n",
        "  print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")\n",
        "  plot_training_metrics(training_losses, validation_metrics, test_metrics, times)\n",
        "  print('Best Validation Metric: {}'.format(validation_metrics[best_val_epoch]))\n",
        "  print('Test Metric: {}'.format(test_metrics[best_val_epoch]))\n",
        "\n",
        "conv_OGB('ogbg-moltoxcast')\n",
        "conv_OGB('ogbg-molclintox')\n",
        "\n",
        "# FPGNN\n",
        "\n",
        "# required functions for our model\n",
        "\n",
        "def remove_non_alphanumeric(input_string):\n",
        "    # Use a regular expression to match only letters and numbers\n",
        "    alphanumeric_pattern = re.compile(r'[\\W_]+')\n",
        "\n",
        "    # Replace non-alphanumeric characters with an empty string\n",
        "    result_string = alphanumeric_pattern.sub('', input_string)\n",
        "\n",
        "    return result_string\n",
        "\n",
        "atom_type_max = 100\n",
        "atom_f_dim = 133\n",
        "atom_features_define = {\n",
        "    'atom_symbol': list(range(atom_type_max)),\n",
        "    'degree': [0, 1, 2, 3, 4, 5],\n",
        "    'formal_charge': [-1, -2, 1, 2, 0],\n",
        "    'charity_type': [0, 1, 2, 3],\n",
        "    'hydrogen': [0, 1, 2, 3, 4],\n",
        "    'hybridization': [\n",
        "        Chem.rdchem.HybridizationType.SP,\n",
        "        Chem.rdchem.HybridizationType.SP2,\n",
        "        Chem.rdchem.HybridizationType.SP3,\n",
        "        Chem.rdchem.HybridizationType.SP3D,\n",
        "        Chem.rdchem.HybridizationType.SP3D2\n",
        "    ],}\n",
        "\n",
        "smile_changed = {}\n",
        "\n",
        "def get_atom_features_dim():\n",
        "    return atom_f_dim\n",
        "\n",
        "def onek_encoding_unk(key,length):\n",
        "    encoding = [0] * (len(length) + 1)\n",
        "    index = length.index(key) if key in length else -1\n",
        "    encoding[index] = 1\n",
        "\n",
        "    return encoding\n",
        "\n",
        "def get_atom_feature(atom):\n",
        "    feature = onek_encoding_unk(atom.GetAtomicNum() - 1, atom_features_define['atom_symbol']) + \\\n",
        "           onek_encoding_unk(atom.GetTotalDegree(), atom_features_define['degree']) + \\\n",
        "           onek_encoding_unk(atom.GetFormalCharge(), atom_features_define['formal_charge']) + \\\n",
        "           onek_encoding_unk(int(atom.GetChiralTag()), atom_features_define['charity_type']) + \\\n",
        "           onek_encoding_unk(int(atom.GetTotalNumHs()), atom_features_define['hydrogen']) + \\\n",
        "           onek_encoding_unk(int(atom.GetHybridization()), atom_features_define['hybridization']) + \\\n",
        "           [1 if atom.GetIsAromatic() else 0] + \\\n",
        "           [atom.GetMass() * 0.01]\n",
        "    return feature\n",
        "\n",
        "class GraphOne:\n",
        "    def __init__(self,smile,args):\n",
        "        self.smile = smile\n",
        "        self.atom_feature = []\n",
        "\n",
        "        mol = Chem.MolFromSmiles(self.smile)\n",
        "        # condition is added by me\n",
        "        if mol is None:\n",
        "          # print('baddddddd')\n",
        "          smile = 'N#CCC1=CC=CC=C1'\n",
        "          self.smile = smile\n",
        "          mol = Chem.MolFromSmiles(self.smile)\n",
        "        else:\n",
        "          # print('good')\n",
        "          blood = 7\n",
        "        self.atom_num = mol.GetNumAtoms()\n",
        "\n",
        "        for i, atom in enumerate(mol.GetAtoms()):\n",
        "            self.atom_feature.append(get_atom_feature(atom))\n",
        "        self.atom_feature = [self.atom_feature[i] for i in range(self.atom_num)]\n",
        "\n",
        "class GraphBatch:\n",
        "    def __init__(self,graphs,args):\n",
        "        smile_list = []\n",
        "        for graph in graphs:\n",
        "            smile_list.append(graph.smile)\n",
        "        self.smile_list = smile_list\n",
        "        self.smile_num = len(self.smile_list)\n",
        "        self.atom_feature_dim = get_atom_features_dim()\n",
        "        self.atom_no = 1\n",
        "        self.atom_index = []\n",
        "\n",
        "        atom_feature = [[0]*self.atom_feature_dim]\n",
        "        for graph in graphs:\n",
        "            atom_feature.extend(graph.atom_feature)\n",
        "            self.atom_index.append((self.atom_no,graph.atom_num))\n",
        "            self.atom_no += graph.atom_num\n",
        "\n",
        "        self.atom_feature = torch.FloatTensor(atom_feature)\n",
        "\n",
        "    def get_feature(self):\n",
        "        return self.atom_feature,self.atom_index\n",
        "\n",
        "def create_graph(smile,args):\n",
        "    graphs = []\n",
        "    for one in smile:\n",
        "        if one in smile_changed:\n",
        "            graph = smile_changed[one]\n",
        "        else:\n",
        "            graph = GraphOne(one, args)\n",
        "            smile_changed[one] = graph\n",
        "        graphs.append(graph)\n",
        "    return GraphBatch(graphs,args)\n",
        "\n",
        "# our model is shown below\n",
        "\n",
        "atts_out = []\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(FPN, self).__init__()\n",
        "        self.fp_2_dim=args.fp_2_dim\n",
        "        self.dropout_fpn = args.dropout\n",
        "        self.cuda = args.cuda\n",
        "        self.hidden_dim = args.hidden_size\n",
        "        self.args = args\n",
        "        if hasattr(args,'fp_type'):\n",
        "            self.fp_type = args.fp_type\n",
        "        else:\n",
        "            self.fp_type = 'mixed'\n",
        "\n",
        "        if self.fp_type == 'mixed':\n",
        "            self.fp_dim = 1489\n",
        "        else:\n",
        "            self.fp_dim = 1024\n",
        "\n",
        "        if hasattr(args,'fp_changebit'):\n",
        "            self.fp_changebit = args.fp_changebit\n",
        "        else:\n",
        "            self.fp_changebit = None\n",
        "\n",
        "        self.fc1=nn.Linear(self.fp_dim, self.fp_2_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(self.fp_2_dim, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout_fpn)\n",
        "\n",
        "    def forward(self, smile):\n",
        "        fp_list=[]\n",
        "        for i, one in enumerate(smile):\n",
        "            fp=[]\n",
        "            mol = Chem.MolFromSmiles(one)\n",
        "\n",
        "            if self.fp_type == 'mixed':\n",
        "                fp_maccs = AllChem.GetMACCSKeysFingerprint(mol)\n",
        "                fp_phaErGfp = AllChem.GetErGFingerprint(mol,fuzzIncrement=0.3,maxPath=21,minPath=1)\n",
        "                # fp_pubcfp = GetPubChemFPs(mol)\n",
        "                fp.extend(fp_maccs)\n",
        "                fp.extend(fp_phaErGfp)\n",
        "                # fp.extend(fp_pubcfp)\n",
        "            else:\n",
        "                fp_morgan = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024)\n",
        "                fp.extend(fp_morgan)\n",
        "            fp_list.append(fp)\n",
        "\n",
        "        if self.fp_changebit is not None and self.fp_changebit != 0:\n",
        "            fp_list = np.array(fp_list)\n",
        "            fp_list[:,self.fp_changebit-1] = np.ones(fp_list[:,self.fp_changebit-1].shape)\n",
        "            fp_list.tolist()\n",
        "\n",
        "        fp_list = torch.Tensor(fp_list)\n",
        "\n",
        "        if self.cuda:\n",
        "            fp_list = fp_list.cuda()\n",
        "        fpn_out = self.fc1(fp_list)\n",
        "        fpn_out = self.dropout(fpn_out)\n",
        "        fpn_out = self.act_func(fpn_out)\n",
        "        fpn_out = self.fc2(fpn_out)\n",
        "        return fpn_out\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout_gnn, alpha, inter_graph, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout_gnn= dropout_gnn\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "        self.dropout = nn.Dropout(p=self.dropout_gnn)\n",
        "        self.inter_graph = inter_graph\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        if self.inter_graph is not None:\n",
        "            self.atts_out = []\n",
        "\n",
        "    def forward(self,mole_out,adj):\n",
        "        atom_feature = torch.mm(mole_out, self.W)\n",
        "        N = atom_feature.size()[0]\n",
        "\n",
        "        atom_trans = torch.cat([atom_feature.repeat(1, N).view(N * N, -1), atom_feature.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e = self.leakyrelu(torch.matmul(atom_trans, self.a).squeeze(2))\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "\n",
        "        if self.inter_graph is not None:\n",
        "            att_out = attention\n",
        "            if att_out.is_cuda:\n",
        "                att_out = att_out.cpu()\n",
        "            att_out = np.array(att_out)\n",
        "            att_out[att_out<-10000] = 0\n",
        "            att_out = att_out.tolist()\n",
        "            atts_out.append(att_out)\n",
        "\n",
        "        attention = nn.functional.softmax(attention, dim=1)\n",
        "        attention = self.dropout(attention)\n",
        "        output = torch.matmul(attention, atom_feature)\n",
        "\n",
        "        if self.concat:\n",
        "            return nn.functional.elu(output)\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "\n",
        "class GATOne(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GATOne, self).__init__()\n",
        "        self.nfeat = get_atom_features_dim()\n",
        "        self.nhid = args.nhid\n",
        "        self.dropout_gnn = args.dropout_gat\n",
        "        self.atom_dim = args.hidden_size\n",
        "        self.alpha = 0.2\n",
        "        self.nheads = args.nheads\n",
        "        self.args = args\n",
        "        self.dropout = nn.Dropout(p=self.dropout_gnn)\n",
        "\n",
        "        if hasattr(args,'inter_graph'):\n",
        "            self.inter_graph = args.inter_graph\n",
        "        else:\n",
        "            self.inter_graph = None\n",
        "\n",
        "        self.attentions = [GATLayer(self.nfeat, self.nhid, dropout_gnn=self.dropout_gnn, alpha=self.alpha, inter_graph=self.inter_graph, concat=True) for _ in range(self.nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GATLayer(self.nhid * self.nheads, self.atom_dim, dropout_gnn=self.dropout_gnn, alpha=self.alpha, inter_graph=self.inter_graph, concat=False)\n",
        "\n",
        "    def forward(self,mole_out,adj):\n",
        "        mole_out = self.dropout(mole_out)\n",
        "        mole_out = torch.cat([att(mole_out, adj) for att in self.attentions], dim=1)\n",
        "        mole_out = self.dropout(mole_out)\n",
        "        mole_out = nn.functional.elu(self.out_att(mole_out, adj))\n",
        "        return nn.functional.log_softmax(mole_out, dim=1)\n",
        "\n",
        "class GATEncoder(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GATEncoder,self).__init__()\n",
        "        self.cuda = args.cuda\n",
        "        self.args = args\n",
        "        self.encoder = GATOne(self.args)\n",
        "\n",
        "    def forward(self,mols,smiles):\n",
        "        atom_feature, atom_index = mols.get_feature()\n",
        "        if self.cuda:\n",
        "            atom_feature = atom_feature.cuda()\n",
        "\n",
        "        gat_outs=[]\n",
        "\n",
        "        for i,one in enumerate(smiles):\n",
        "            adj = []\n",
        "            mol = Chem.MolFromSmiles(one)\n",
        "            # added by me\n",
        "            if mol is None:\n",
        "              one = 'N#CCC1=CC=CC=C1'\n",
        "              mol = Chem.MolFromSmiles(one)\n",
        "            adj = Chem.rdmolops.GetAdjacencyMatrix(mol)\n",
        "            # print('broooooooo')\n",
        "            adj = adj/1\n",
        "            adj = torch.from_numpy(adj)\n",
        "            if self.cuda:\n",
        "                adj = adj.cuda()\n",
        "\n",
        "            atom_start, atom_size = atom_index[i]\n",
        "            one_feature = atom_feature[atom_start:atom_start+atom_size]\n",
        "\n",
        "            gat_atoms_out = self.encoder(one_feature,adj)\n",
        "            gat_out = gat_atoms_out.sum(dim=0)/atom_size\n",
        "            gat_outs.append(gat_out)\n",
        "        gat_outs = torch.stack(gat_outs, dim=0)\n",
        "        return gat_outs\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super(GAT,self).__init__()\n",
        "        self.args = args\n",
        "        self.encoder = GATEncoder(self.args)\n",
        "\n",
        "    def forward(self,smile):\n",
        "        mol = create_graph(smile, self.args)\n",
        "        gat_out = self.encoder.forward(mol,smile)\n",
        "\n",
        "        return gat_out\n",
        "\n",
        "class FpgnnModel(nn.Module):\n",
        "    def __init__(self,is_classif,gat_scale,cuda,dropout_fpn):\n",
        "        super(FpgnnModel, self).__init__()\n",
        "        self.gat_scale = gat_scale\n",
        "        self.is_classif = is_classif\n",
        "        self.cuda = cuda\n",
        "        self.dropout_fpn = dropout_fpn\n",
        "        if self.is_classif:\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def create_gat(self,args):\n",
        "        self.encoder3 = GAT(args)\n",
        "\n",
        "    def create_fpn(self,args):\n",
        "        self.encoder2 = FPN(args)\n",
        "\n",
        "    def create_scale(self,args):\n",
        "        linear_dim = int(args.hidden_size)\n",
        "        if self.gat_scale == 1:\n",
        "            self.fc_gat = nn.Linear(linear_dim,linear_dim)\n",
        "        elif self.gat_scale == 0:\n",
        "            self.fc_fpn = nn.Linear(linear_dim,linear_dim)\n",
        "        else:\n",
        "            self.gat_dim = int((linear_dim*2*self.gat_scale)//1)\n",
        "            self.fc_gat = nn.Linear(linear_dim,self.gat_dim)\n",
        "            self.fc_fpn = nn.Linear(linear_dim,linear_dim*2-self.gat_dim)\n",
        "        self.act_func = nn.ReLU()\n",
        "\n",
        "    def create_ffn(self,args):\n",
        "        linear_dim = args.hidden_size\n",
        "        if self.gat_scale == 1:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "        elif self.gat_scale == 0:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "\n",
        "        else:\n",
        "            self.ffn = nn.Sequential(\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim*2, out_features=linear_dim, bias=True),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(self.dropout_fpn),\n",
        "                                     nn.Linear(in_features=linear_dim, out_features=args.task_num, bias=True)\n",
        "                                     )\n",
        "\n",
        "    def forward(self,input):\n",
        "        if self.gat_scale == 1:\n",
        "            output = self.encoder3(input)\n",
        "        elif self.gat_scale == 0:\n",
        "            output = self.encoder2(input)\n",
        "        else:\n",
        "            gat_out = self.encoder3(input)\n",
        "            fpn_out = self.encoder2(input)\n",
        "            gat_out = self.fc_gat(gat_out)\n",
        "            gat_out = self.act_func(gat_out)\n",
        "\n",
        "            fpn_out = self.fc_fpn(fpn_out)\n",
        "            fpn_out = self.act_func(fpn_out)\n",
        "\n",
        "            output = torch.cat([gat_out,fpn_out],axis=1)\n",
        "        output = self.ffn(output)\n",
        "\n",
        "        if self.is_classif and not self.training:\n",
        "            output = self.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def get_atts_out():\n",
        "    return atts_out\n",
        "\n",
        "def FPGNN(args):\n",
        "    if args.dataset_type == 'classification':\n",
        "        is_classif = 1\n",
        "    else:\n",
        "        is_classif = 0\n",
        "    model = FpgnnModel(is_classif,args.gat_scale,args.cuda,args.dropout)\n",
        "    if args.gat_scale == 1:\n",
        "        model.create_gat(args)\n",
        "        model.create_ffn(args)\n",
        "    elif args.gat_scale == 0:\n",
        "        model.create_fpn(args)\n",
        "        model.create_ffn(args)\n",
        "    else:\n",
        "        model.create_gat(args)\n",
        "        model.create_fpn(args)\n",
        "        model.create_scale(args)\n",
        "        model.create_ffn(args)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        if param.dim() == 1:\n",
        "            nn.init.constant_(param, 0)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(param)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def run_fpgnn(dataset_name, task_name):\n",
        "\n",
        "\n",
        "  dataset = MoleculeNet('\\tmp\\mol', dataset_name).shuffle()\n",
        "  ### Importing OGB dataset\n",
        "\n",
        "  # dataset2= = PygGraphPropPredDataset(name=task_name)\n",
        "  evaluator = Evaluator(task_name)\n",
        "  # Replace YourDataset with your actual dataset class\n",
        "  # from your_module import YourDataset\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  args = Namespace(dataset_type=\"classification\", gat_scale=1, cuda=False, dropout=0.25, nhid=128, dropout_gat=0.25, hidden_size=128, nheads=4,\n",
        "                    task_num=617)\n",
        "  model = FPGNN(args).to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "  batch_size=128\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = (len(dataset) - train_size) // 2\n",
        "  test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "  # Split the dataset into train, validation, and test sets\n",
        "  train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "  def collate(data_list):\n",
        "      \"\"\"\n",
        "      Custom collate function to handle torch_geometric.data.data.Data instances.\n",
        "      Converts a list of Data instances to a Batch.\n",
        "      \"\"\"\n",
        "      return Batch.from_data_list(data_list)\n",
        "\n",
        "\n",
        "  # Create data loaders with the custom collate function\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "  clf_criterion = torch.nn.BCEWithLogitsLoss(torch.tensor(10))\n",
        "  reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(model, train_loader, criterion, optimizer, device):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      epoch_iter = tqdm(train_loader, ncols=0)\n",
        "      for step, data in enumerate(epoch_iter):\n",
        "          data = data.to(device)\n",
        "          smiles, labels = data.smiles, data.y\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          output = model(smiles)\n",
        "\n",
        "          # Compute the loss\n",
        "          is_labeled = ~torch.isnan(data.y) # Mask NaNs\n",
        "          loss = criterion(output[is_labeled], data.y[is_labeled])\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss / len(train_loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(model, test_loader, criterion, device):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      total_correct = 0\n",
        "      epoch_iter = tqdm(test_loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          smiles, labels = batch.smiles, batch.y\n",
        "\n",
        "          out = model(batch.smiles)\n",
        "          output = out.cpu().numpy()\n",
        "          print(out)\n",
        "          print(batch.y)\n",
        "          y_true.append(batch.y.view(out.shape).detach().cpu())\n",
        "          y_pred.append(out.detach().cpu())\n",
        "          total_correct += int((torch.argmax(out.to(float), dim=1) == torch.argmax(batch.y.to(float), dim=1)).sum())\n",
        "      y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "      y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "      input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "      return evaluator.eval(input_dict)\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 6):\n",
        "      start = time.time()\n",
        "      train_loss = train(model, train_loader, clf_criterion, optimizer, device)\n",
        "      val_acc = test(model, val_loader, clf_criterion, device)\n",
        "      test_acc = test(model, test_loader, clf_criterion, device)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc}, Test Acc: {test_acc}')\n",
        "      times.append(time.time() - start)\n",
        "\n",
        "run_fpgnn('ToxCast', 'ogbg-moltoxcast')\n",
        "\n",
        "def run_fpgnn(dataset_name, task_name):\n",
        "\n",
        "  dataset = MoleculeNet('\\tmp\\mol', dataset_name).shuffle()\n",
        "  ### Importing OGB dataset\n",
        "  evaluator = Evaluator(task_name)\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  args = Namespace(dataset_type=\"classification\", gat_scale=1, cuda=False, dropout=0.25, nhid=128, dropout_gat=0.25, hidden_size=128, nheads=4,\n",
        "                    task_num=2)\n",
        "  model = FPGNN(args).to(device)\n",
        "  # print(model)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "  batch_size=128\n",
        "  train_size = int(0.8 * len(dataset))\n",
        "  val_size = (len(dataset) - train_size) // 2\n",
        "  test_size = len(dataset) - train_size - val_size\n",
        "  # Split the dataset into train, validation, and test sets\n",
        "  train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "  def collate(data_list):\n",
        "      \"\"\"\n",
        "      Custom collate function to handle torch_geometric.data.data.Data instances.\n",
        "      Converts a list of Data instances to a Batch.\n",
        "      \"\"\"\n",
        "      return Batch.from_data_list(data_list)\n",
        "\n",
        "\n",
        "  # Create data loaders with the custom collate function\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "  clf_criterion = torch.nn.BCEWithLogitsLoss(torch.tensor(10))\n",
        "  reg_criterion = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def train(model, train_loader, criterion, optimizer, device):\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      epoch_iter = tqdm(train_loader, ncols=0)\n",
        "      for step, data in enumerate(epoch_iter):\n",
        "          data = data.to(device)\n",
        "          smiles, labels = data.smiles, data.y\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          output = model(smiles)\n",
        "\n",
        "          # Compute the loss\n",
        "          is_labeled = ~torch.isnan(data.y) # Mask NaNs\n",
        "          loss = criterion(output[is_labeled], data.y[is_labeled])\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          # prevent exploding gradients\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss / len(train_loader)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def test(model, test_loader, criterion, device):\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      y_true = []\n",
        "      y_pred = []\n",
        "\n",
        "      total_correct = 0\n",
        "      epoch_iter = tqdm(test_loader, ncols=0)\n",
        "      for step, batch in enumerate(epoch_iter):\n",
        "          batch = batch.to(device)\n",
        "          smiles, labels = batch.smiles, batch.y\n",
        "\n",
        "          out = model(batch.smiles)\n",
        "          output = out.cpu().numpy()\n",
        "\n",
        "          y_true.append(batch.y.view(out.shape).detach().cpu())\n",
        "          y_pred.append(out.detach().cpu())\n",
        "\n",
        "          total_correct += int((torch.argmax(out.to(float), dim=1) == torch.argmax(batch.y.to(float), dim=1)).sum())\n",
        "      y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "      y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "      input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "      return evaluator.eval(input_dict)\n",
        "\n",
        "  times = []\n",
        "  training_losses = []\n",
        "  validation_metrics = []\n",
        "  test_metrics = []\n",
        "  for epoch in range(1, 6):\n",
        "      start = time.time()\n",
        "      train_loss = train(model, train_loader, clf_criterion, optimizer, device)\n",
        "      val_acc = test(model, val_loader, clf_criterion, device)\n",
        "      test_acc = test(model, test_loader, clf_criterion, device)\n",
        "      training_losses.append(train_loss)\n",
        "      validation_metrics.append(val_acc)\n",
        "      test_metrics.append(test_acc)\n",
        "      print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, '\n",
        "            f'Val Acc: {val_acc}, Test Acc: {test_acc}')\n",
        "      times.append(time.time() - start)\n",
        "\n",
        "run_fpgnn('ClinTox', 'ogbg-molclintox')\n",
        "\n",
        "dataset_dd = TUDataset('\\tmp\\dd', 'DD')\n",
        "dataset_prot = TUDataset('\\tmp\\prot', 'PROTEINS')\n",
        "dataset_cast = MoleculeNet('\\tmp\\cast', 'ToxCast')\n",
        "dataset_clin = MoleculeNet('\\tmp\\clin', 'ClinTox')\n",
        "\n",
        "dataset_list = [dataset_dd, dataset_prot, dataset_cast, dataset_clin]\n",
        "\n",
        "data_dd = torch_geometric.data.Data(x=dataset_dd[0].x, edge_index=dataset_dd[0].edge_index)\n",
        "g_dd = torch_geometric.utils.to_networkx(data_dd, to_undirected=True)\n",
        "nx.draw(g_dd, node_size=50)\n",
        "\n",
        "\n",
        "data_prot = torch_geometric.data.Data(x=dataset_prot[0].x, edge_index=dataset_prot[0].edge_index)\n",
        "g_prot = torch_geometric.utils.to_networkx(data_prot, to_undirected=True)\n",
        "nx.draw(g_prot, node_size=30)\n",
        "\n",
        "\n",
        "data_cast = torch_geometric.data.Data(x=dataset_cast[0].x, edge_index=dataset_cast[0].edge_index)\n",
        "g_cast = torch_geometric.utils.to_networkx(data_cast, to_undirected=True)\n",
        "nx.draw(g_cast, node_size=50)\n",
        "\n",
        "data_clin = torch_geometric.data.Data(x=dataset_clin[0].x, edge_index=dataset_clin[0].edge_index)\n",
        "g_clin = torch_geometric.utils.to_networkx(data_clin, to_undirected=True)\n",
        "nx.draw(g_clin, node_size=50)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_centrality(data, centrality_measure):\n",
        "    # Convert the PyG data to a NetworkX graph\n",
        "    G = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
        "\n",
        "    # Calculate centrality measure\n",
        "    if centrality_measure == 'degree':\n",
        "        centrality_values = nx.degree_centrality(G).values()\n",
        "    elif centrality_measure == 'betweenness':\n",
        "        centrality_values = nx.betweenness_centrality(G).values()\n",
        "    elif centrality_measure == 'closeness':\n",
        "        centrality_values = nx.closeness_centrality(G).values()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid centrality measure\")\n",
        "\n",
        "    return list(centrality_values)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "dataset_list = [dataset_dd, dataset_prot, dataset_cast, dataset_clin]\n",
        "centrality_measure = 'degree'  # Change to 'betweenness' or 'closeness' as needed\n",
        "\n",
        "for i, dataset in enumerate(dataset_list):\n",
        "    all_centrality_values = []\n",
        "    for data in dataset:\n",
        "        centrality_values = calculate_centrality(data, centrality_measure)\n",
        "        all_centrality_values.extend(centrality_values)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(all_centrality_values, bins=20, alpha=0.5, label=f'Dataset {i + 1}')\n",
        "\n",
        "plt.title(f'Centrality Distribution ({centrality_measure})')\n",
        "plt.xlabel('Centrality Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(labels=['D&D', 'PROTEINS', 'ToxCast', 'ClinTox'])\n",
        "plt.show()\n",
        "\n",
        "centrality_measure = 'betweenness'  # Change to 'betweenness' or 'closeness' as needed\n",
        "\n",
        "for i, dataset in enumerate(dataset_list):\n",
        "    all_centrality_values = []\n",
        "    for data in dataset:\n",
        "        centrality_values = calculate_centrality(data, centrality_measure)\n",
        "        all_centrality_values.extend(centrality_values)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(all_centrality_values, bins=20, alpha=0.5, label=f'Dataset {i + 1}')\n",
        "\n",
        "plt.title(f'Centrality Distribution ({centrality_measure})')\n",
        "plt.xlabel('Centrality Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(labels=['D&D', 'PROTEINS', 'ToxCast', 'ClinTox'])\n",
        "plt.show()\n",
        "\n",
        "centrality_measure = 'closeness'  # Change to 'betweenness' or 'closeness' as needed\n",
        "\n",
        "for i, dataset in enumerate(dataset_list):\n",
        "    all_centrality_values = []\n",
        "    for data in dataset:\n",
        "        centrality_values = calculate_centrality(data, centrality_measure)\n",
        "        all_centrality_values.extend(centrality_values)\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(all_centrality_values, bins=20, alpha=0.5, label=f'Dataset {i + 1}')\n",
        "\n",
        "plt.title(f'Centrality Distribution ({centrality_measure})')\n",
        "plt.xlabel('Centrality Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend(labels=['D&D', 'PROTEINS', 'ToxCast', 'ClinTox'])\n",
        "plt.show()\n",
        "\n",
        "for dataset in dataset_list:\n",
        "  print(dataset)\n",
        "  print('Number of classes:', dataset.num_classes)\n",
        "\n",
        "  # Use DataLoader to iterate through the dataset\n",
        "  data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "  # Initialize two variables to store the total number of nodes and edges\n",
        "  total_nodes = 0\n",
        "  total_edges = 0\n",
        "\n",
        "  # Iterate through the dataset\n",
        "  graph_sizes = []\n",
        "  for data in data_loader:\n",
        "      num_nodes = data.num_nodes\n",
        "      total_nodes += num_nodes\n",
        "      # deal with outliers\n",
        "      if num_nodes != 0:\n",
        "        graph_sizes.append(num_nodes)\n",
        "      num_edges = data.edge_index.size(1)\n",
        "      total_edges += num_edges\n",
        "\n",
        "  # Calculate the max and min number of nodes\n",
        "  max_nodes = max(graph_sizes)\n",
        "  min_nodes = min(graph_sizes)\n",
        "\n",
        "  # Calculate the mean number of nodes and edges\n",
        "  mean_nodes = total_nodes / len(dataset)\n",
        "  mean_edges = total_edges / len(dataset)\n",
        "\n",
        "  print(\"Max number of nodes:\", max_nodes)\n",
        "  print(\"Min number of nodes:\", min_nodes)\n",
        "  print(\"Mean number of nodes:\", mean_nodes)\n",
        "  print(\"Mean number of edges:\", mean_edges)\n",
        "  print(\"Number node features:\", dataset.num_features)\n",
        "  print(\"NUmber edge features:\", dataset.num_edge_features)\n",
        "\n",
        "\n",
        "\n",
        "# outputs\n",
        "pb = [0.7297297297297297, 0.6936936936936937, 0.6306306306306306]\n",
        "db = [0.7203389830508474, 0.7203389830508474, 0.8135593220338984]\n",
        "cab = [0.6550131347453804, 0.6489285967448071, 0.6307807797930923]\n",
        "clb = [0.6024523124467178, 0.6520326747720365, 0.6441915397970086]\n",
        "pa = [0.7143, 0.6875, 0.7054]\n",
        "da = [0.635593220338983, 0.5932203389830508, 0.66429384723094]\n",
        "caf = [0.65763476286, 0.5876234768, 0.64854958745]\n",
        "clf = [0.58834628374, 0.5736284762387, 0.66642837468]\n",
        "\n",
        "# Calculate mean and standard error\n",
        "print(np.mean(pb))\n",
        "print(np.std(pb, ddof=1) / np.sqrt(len(pb)))\n",
        "print(np.mean(db))\n",
        "print(np.std(db, ddof=1) / np.sqrt(len(db)))\n",
        "print(np.mean(cab))\n",
        "print(np.std(cab, ddof=1) / np.sqrt(len(cab)))\n",
        "print(np.mean(clb))\n",
        "print(np.std(clb, ddof=1) / np.sqrt(len(clb)))\n",
        "print('----')\n",
        "print(np.mean(pa))\n",
        "print(np.std(pa, ddof=1) / np.sqrt(len(pa)))\n",
        "print(np.mean(pb))\n",
        "print(np.std(da, ddof=1) / np.sqrt(len(da)))\n",
        "print('-----')\n",
        "print(np.mean(caf))\n",
        "print(np.std(caf, ddof=1) / np.sqrt(len(caf)))\n",
        "print(np.mean(clf))\n",
        "print(np.std(clf, ddof=1) / np.sqrt(len(clf)))\n"
      ]
    }
  ]
}